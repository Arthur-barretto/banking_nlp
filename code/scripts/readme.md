# Local LLM Processing and Evaluation Pipeline

## Overview
This project processes and evaluates text files using local LLMs. The workflow supports both **Llama** and **Qwen** models, with identical steps and purposes for each. The pipeline includes three main stages:
1. Generating topics and sentiment evaluations.
2. Comparing and grading responses from different LLMs.
3. Aggregating evaluations into model-level assessments with strengths, weaknesses, and recommendations.

The workflow is structured to adapt seamlessly to both Llama and Qwen, with only the model-specific filenames differing between the two.

---

## Workflow Description (Applies to Both Llama and Qwen)

### 1. Topic and Sentiment Extraction
The first stage processes text files to extract meaningful topics and evaluate sentiment using the LLM.

- **Tasks:**
  - **Task 1:** Extract up to 10 key topics from the input text in bullet-point format.
  - **Task 2:** Perform a sentiment evaluation (`"positivo"` or `"negativo"`).
- **Script:**
  - For Llama: `LLM_llama3_unsupervised.py`
  - For Qwen: `LLM_qwen_unsupervised.py`
- **Input Folder:** `/home/arthurblb/mestrado/Divided_text/qna/`
- **Output Folders:**
  - Llama: `/home/arthurblb/mestrado/Divided_text/output/llama/unsupervised/`
  - Qwen: `/home/arthurblb/mestrado/Divided_text/output/qwen/unsupervised/`

---

### 2. Model Evaluation
In the second stage, the models evaluate the responses generated by other models, comparing outputs to the original task prompt and text.

- **Tasks:**
  - Score and explain the performance of other models based on:
    - Adherence to the task prompt.
    - Relevance of extracted topics.
    - Compliance with the requested format.
- **Script:**
  - For Llama evaluating Qwen and ChatGPT: `LLM_as_a_judge_llama_contextualized.py`
  - For Qwen evaluating Llama and ChatGPT: `LLM_as_a_judge_qwen_contextualized.py`
- **Input Folders:**
  - Original Text: `/home/arthurblb/mestrado/Divided_text/qna/`
  - Model Outputs:
    - Llama: `/home/arthurblb/mestrado/Divided_text/output/llama/unsupervised/`
    - Qwen: `/home/arthurblb/mestrado/Divided_text/output/qwen/unsupervised/`
    - ChatGPT: `/home/arthurblb/mestrado/Divided_text/output/chatgpt/unsupervised/`
- **Output Folders:**
  - Llama: `/home/arthurblb/mestrado/Divided_text/output/results/judge_llama/contextualized/`
  - Qwen: `/home/arthurblb/mestrado/Divided_text/output/results/judge_qwen/contextualized/`

---

### 3. Model-Level Assessment
The final stage aggregates evaluations to generate an overall assessment of each model's performance.

- **Tasks:**
  - Summarize:
    - Strengths.
    - Weaknesses.
    - Recommendations for improvement.
    - General performance.
- **Script:**
  - For Llama: `judge_llama_model_assessment.py`
  - For Qwen: `judge_qwen_model_assessment.py`
- **Input Folders:**
  - Llama: `/home/arthurblb/mestrado/Divided_text/output/results/judge_llama/contextualized/`
  - Qwen: `/home/arthurblb/mestrado/Divided_text/output/results/judge_qwen/contextualized/`
- **Output Folders:**
  - Llama: `/home/arthurblb/mestrado/Divided_text/output/results/judge_llama/model_assessments/`
  - Qwen: `/home/arthurblb/mestrado/Divided_text/output/results/judge_qwen/model_assessments/`

---

## Key Notes
- **Identical Workflow:** The steps and structure remain the same for both Llama and Qwen. Only filenames and output directories differ based on the model used.
- **Extensibility:** The design supports adding other models to the pipeline in the future.
