{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWAQ5RCU7OiP"
   },
   "source": [
    "# Generate document-level assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 8398,
     "status": "ok",
     "timestamp": 1736073702317,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "YsCvubVNWrVZ",
    "outputId": "39c9d7da-c7b3-42d1-d2e9-342c63aee5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "Successfully installed openai-0.28.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "8131a9fe0dd840788438aefb19a07adf",
       "pip_warning": {
        "packages": [
         "openai"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2180,
     "status": "ok",
     "timestamp": 1736073726249,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "ShW7uvHYLspY"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6223,
     "status": "ok",
     "timestamp": 1736073708535,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "KgpMVBD4WQWN",
    "outputId": "e500e928-3376-4e98-a493-b26ce1f07b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 0.28.0\n",
      "Summary: Python client library for the OpenAI API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: OpenAI\n",
      "Author-email: support@openai.com\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, requests, tqdm\n",
      "Required-by: \n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29280,
     "status": "ok",
     "timestamp": 1736073761758,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "w_IxpXrSLuUA",
    "outputId": "6f13c1fe-1e80-414b-828a-594b95f5a1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4780768,
     "status": "ok",
     "timestamp": 1735985598792,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "hv0S91gbJQJF",
    "outputId": "3f656dff-0c40-41b7-eeff-db7754733b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file prbc-2012-1\n",
      "1/375\n",
      "Evaluation for prbc-2012-1.txt completed. Results saved.\n",
      "Processing file bmgb-2022-2\n",
      "2/375\n",
      "Evaluation for bmgb-2022-2.txt completed. Results saved.\n",
      "Processing file brsr-2013-4\n",
      "3/375\n",
      "Evaluation for brsr-2013-4.txt completed. Results saved.\n",
      "Processing file abcb-2022-3\n",
      "4/375\n",
      "Evaluation for abcb-2022-3.txt completed. Results saved.\n",
      "Processing file brsr-2007-4\n",
      "5/375\n",
      "Evaluation for brsr-2007-4.txt completed. Results saved.\n",
      "Processing file abcb-2010-2\n",
      "6/375\n",
      "Evaluation for abcb-2010-2.txt completed. Results saved.\n",
      "Processing file prbc-2010-2\n",
      "7/375\n",
      "Evaluation for prbc-2010-2.txt completed. Results saved.\n",
      "Processing file bbas-2006-4\n",
      "8/375\n",
      "Evaluation for bbas-2006-4.txt completed. Results saved.\n",
      "Processing file bbas-2019-2\n",
      "9/375\n",
      "Evaluation for bbas-2019-2.txt completed. Results saved.\n",
      "Processing file sanb-2015-3\n",
      "10/375\n",
      "Evaluation for sanb-2015-3.txt completed. Results saved.\n",
      "Processing file itub-2018-4\n",
      "11/375\n",
      "Evaluation for itub-2018-4.txt completed. Results saved.\n",
      "Processing file bbas-2009-1\n",
      "12/375\n",
      "Evaluation for bbas-2009-1.txt completed. Results saved.\n",
      "Processing file brsr-2019-3\n",
      "13/375\n",
      "Evaluation for brsr-2019-3.txt completed. Results saved.\n",
      "Processing file brsr-2010-3\n",
      "14/375\n",
      "Evaluation for brsr-2010-3.txt completed. Results saved.\n",
      "Processing file bbas-2018-2\n",
      "15/375\n",
      "Evaluation for bbas-2018-2.txt completed. Results saved.\n",
      "Processing file prbc-2010-1\n",
      "16/375\n",
      "Error evaluating response: Invalid control character at: line 1 column 594 (char 593)\n",
      "Evaluation for prbc-2010-1.txt completed. Results saved.\n",
      "Processing file abcb-2013-3\n",
      "17/375\n",
      "Evaluation for abcb-2013-3.txt completed. Results saved.\n",
      "Processing file bbas-2020-1\n",
      "18/375\n",
      "Evaluation for bbas-2020-1.txt completed. Results saved.\n",
      "Processing file sanb-2019-3\n",
      "19/375\n",
      "Evaluation for sanb-2019-3.txt completed. Results saved.\n",
      "Processing file bbdc-2021-1\n",
      "20/375\n",
      "Evaluation for bbdc-2021-1.txt completed. Results saved.\n",
      "Processing file brsr-2019-1\n",
      "21/375\n",
      "Evaluation for brsr-2019-1.txt completed. Results saved.\n",
      "Processing file sanb-2011-2\n",
      "22/375\n",
      "Evaluation for sanb-2011-2.txt completed. Results saved.\n",
      "Processing file sanb-2022-2\n",
      "23/375\n",
      "Evaluation for sanb-2022-2.txt completed. Results saved.\n",
      "Processing file bbas-2007-4\n",
      "24/375\n",
      "Evaluation for bbas-2007-4.txt completed. Results saved.\n",
      "Processing file bbdc-2019-4\n",
      "25/375\n",
      "Evaluation for bbdc-2019-4.txt completed. Results saved.\n",
      "Processing file bbdc-2022-2\n",
      "26/375\n",
      "Evaluation for bbdc-2022-2.txt completed. Results saved.\n",
      "Processing file bpan-2018-1\n",
      "27/375\n",
      "Error evaluating response: Invalid control character at: line 1 column 531 (char 530)\n",
      "Evaluation for bpan-2018-1.txt completed. Results saved.\n",
      "Processing file abcb-2014-3\n",
      "28/375\n",
      "Evaluation for abcb-2014-3.txt completed. Results saved.\n",
      "Processing file brsr-2018-4\n",
      "29/375\n",
      "Evaluation for brsr-2018-4.txt completed. Results saved.\n",
      "Processing file abcb-2009-1\n",
      "30/375\n",
      "Evaluation for abcb-2009-1.txt completed. Results saved.\n",
      "Processing file brsr-2021-3\n",
      "31/375\n",
      "Evaluation for brsr-2021-3.txt completed. Results saved.\n",
      "Processing file abcb-2008-4\n",
      "32/375\n",
      "Evaluation for abcb-2008-4.txt completed. Results saved.\n",
      "Processing file abcb-2018-1\n",
      "33/375\n",
      "Evaluation for abcb-2018-1.txt completed. Results saved.\n",
      "Processing file itub-2021-3\n",
      "34/375\n",
      "Evaluation for itub-2021-3.txt completed. Results saved.\n",
      "Processing file abcb-2010-4\n",
      "35/375\n",
      "Evaluation for abcb-2010-4.txt completed. Results saved.\n",
      "Processing file brsr-2014-3\n",
      "36/375\n",
      "Evaluation for brsr-2014-3.txt completed. Results saved.\n",
      "Processing file bbas-2007-2\n",
      "37/375\n",
      "Evaluation for bbas-2007-2.txt completed. Results saved.\n",
      "Processing file abcb-2016-3\n",
      "38/375\n",
      "Evaluation for abcb-2016-3.txt completed. Results saved.\n",
      "Processing file sanb-2011-3\n",
      "39/375\n",
      "Evaluation for sanb-2011-3.txt completed. Results saved.\n",
      "Processing file itub-2016-1\n",
      "40/375\n",
      "Evaluation for itub-2016-1.txt completed. Results saved.\n",
      "Processing file brsr-2007-2\n",
      "41/375\n",
      "Evaluation for brsr-2007-2.txt completed. Results saved.\n",
      "Processing file bbas-2017-4\n",
      "42/375\n",
      "Evaluation for bbas-2017-4.txt completed. Results saved.\n",
      "Processing file sanb-2020-4\n",
      "43/375\n",
      "Evaluation for sanb-2020-4.txt completed. Results saved.\n",
      "Processing file bbdc-2019-1\n",
      "44/375\n",
      "Evaluation for bbdc-2019-1.txt completed. Results saved.\n",
      "Processing file bbdc-2019-2\n",
      "45/375\n",
      "Evaluation for bbdc-2019-2.txt completed. Results saved.\n",
      "Processing file bpan-2017-4\n",
      "46/375\n",
      "Evaluation for bpan-2017-4.txt completed. Results saved.\n",
      "Processing file sanb-2016-2\n",
      "47/375\n",
      "Evaluation for sanb-2016-2.txt completed. Results saved.\n",
      "Processing file sanb-2013-3\n",
      "48/375\n",
      "Evaluation for sanb-2013-3.txt completed. Results saved.\n",
      "Processing file abcb-2020-4\n",
      "49/375\n",
      "Evaluation for abcb-2020-4.txt completed. Results saved.\n",
      "Processing file sanb-2015-1\n",
      "50/375\n",
      "Evaluation for sanb-2015-1.txt completed. Results saved.\n",
      "Processing file bpan-2012-4\n",
      "51/375\n",
      "Evaluation for bpan-2012-4.txt completed. Results saved.\n",
      "Processing file brsr-2011-4\n",
      "52/375\n",
      "Evaluation for brsr-2011-4.txt completed. Results saved.\n",
      "Processing file bbas-2011-4\n",
      "53/375\n",
      "Evaluation for bbas-2011-4.txt completed. Results saved.\n",
      "Processing file brsr-2022-1\n",
      "54/375\n",
      "Evaluation for brsr-2022-1.txt completed. Results saved.\n",
      "Processing file sanb-2014-3\n",
      "55/375\n",
      "Evaluation for sanb-2014-3.txt completed. Results saved.\n",
      "Processing file sanb-2010-3\n",
      "56/375\n",
      "Evaluation for sanb-2010-3.txt completed. Results saved.\n",
      "Processing file nu-2022-4\n",
      "57/375\n",
      "Evaluation for nu-2022-4.txt completed. Results saved.\n",
      "Processing file bbas-2011-3\n",
      "58/375\n",
      "Evaluation for bbas-2011-3.txt completed. Results saved.\n",
      "Processing file itub-2017-1\n",
      "59/375\n",
      "Evaluation for itub-2017-1.txt completed. Results saved.\n",
      "Processing file itub-2017-2\n",
      "60/375\n",
      "Evaluation for itub-2017-2.txt completed. Results saved.\n",
      "Processing file itub-2017-3\n",
      "61/375\n",
      "Evaluation for itub-2017-3.txt completed. Results saved.\n",
      "Processing file abcb-2015-4\n",
      "62/375\n",
      "Evaluation for abcb-2015-4.txt completed. Results saved.\n",
      "Processing file itub-2015-1\n",
      "63/375\n",
      "Evaluation for itub-2015-1.txt completed. Results saved.\n",
      "Processing file abcb-2013-1\n",
      "64/375\n",
      "Evaluation for abcb-2013-1.txt completed. Results saved.\n",
      "Processing file abcb-2022-4\n",
      "65/375\n",
      "Evaluation for abcb-2022-4.txt completed. Results saved.\n",
      "Processing file brsr-2017-2\n",
      "66/375\n",
      "Evaluation for brsr-2017-2.txt completed. Results saved.\n",
      "Processing file bpan-2020-1\n",
      "67/375\n",
      "Evaluation for bpan-2020-1.txt completed. Results saved.\n",
      "Processing file brsr-2008-3\n",
      "68/375\n",
      "Evaluation for brsr-2008-3.txt completed. Results saved.\n",
      "Processing file bbdc-2022-1\n",
      "69/375\n",
      "Evaluation for bbdc-2022-1.txt completed. Results saved.\n",
      "Processing file bbdc-2018-1\n",
      "70/375\n",
      "Evaluation for bbdc-2018-1.txt completed. Results saved.\n",
      "Processing file itub-2016-2\n",
      "71/375\n",
      "Evaluation for itub-2016-2.txt completed. Results saved.\n",
      "Processing file brsr-2018-3\n",
      "72/375\n",
      "Evaluation for brsr-2018-3.txt completed. Results saved.\n",
      "Processing file bbas-2010-4\n",
      "73/375\n",
      "Evaluation for bbas-2010-4.txt completed. Results saved.\n",
      "Processing file bpan-2017-2\n",
      "74/375\n",
      "Evaluation for bpan-2017-2.txt completed. Results saved.\n",
      "Processing file bbas-2013-1\n",
      "75/375\n",
      "Evaluation for bbas-2013-1.txt completed. Results saved.\n",
      "Processing file itub-2023-2\n",
      "76/375\n",
      "Evaluation for itub-2023-2.txt completed. Results saved.\n",
      "Processing file sanb-2014-1\n",
      "77/375\n",
      "Evaluation for sanb-2014-1.txt completed. Results saved.\n",
      "Processing file nu-2023-2\n",
      "78/375\n",
      "Evaluation for nu-2023-2.txt completed. Results saved.\n",
      "Processing file brsr-2015-4\n",
      "79/375\n",
      "Evaluation for brsr-2015-4.txt completed. Results saved.\n",
      "Processing file brsr-2007-3\n",
      "80/375\n",
      "Evaluation for brsr-2007-3.txt completed. Results saved.\n",
      "Processing file prbc-2010-4\n",
      "81/375\n",
      "Evaluation for prbc-2010-4.txt completed. Results saved.\n",
      "Processing file abcb-2011-3\n",
      "82/375\n",
      "Evaluation for abcb-2011-3.txt completed. Results saved.\n",
      "Processing file brsr-2011-3\n",
      "83/375\n",
      "Evaluation for brsr-2011-3.txt completed. Results saved.\n",
      "Processing file bpan-2020-3\n",
      "84/375\n",
      "Evaluation for bpan-2020-3.txt completed. Results saved.\n",
      "Processing file itub-2011-2\n",
      "85/375\n",
      "Evaluation for itub-2011-2.txt completed. Results saved.\n",
      "Processing file abcb-2011-2\n",
      "86/375\n",
      "Evaluation for abcb-2011-2.txt completed. Results saved.\n",
      "Processing file bbdc-2017-3\n",
      "87/375\n",
      "Evaluation for bbdc-2017-3.txt completed. Results saved.\n",
      "Processing file itub-2013-2\n",
      "88/375\n",
      "Evaluation for itub-2013-2.txt completed. Results saved.\n",
      "Processing file brsr-2010-1\n",
      "89/375\n",
      "Evaluation for brsr-2010-1.txt completed. Results saved.\n",
      "Processing file bmgb-2023-1\n",
      "90/375\n",
      "Evaluation for bmgb-2023-1.txt completed. Results saved.\n",
      "Processing file sanb-2019-1\n",
      "91/375\n",
      "Evaluation for sanb-2019-1.txt completed. Results saved.\n",
      "Processing file itub-2020-2\n",
      "92/375\n",
      "Evaluation for itub-2020-2.txt completed. Results saved.\n",
      "Processing file itub-2020-1\n",
      "93/375\n",
      "Evaluation for itub-2020-1.txt completed. Results saved.\n",
      "Processing file bmgb-2020-2\n",
      "94/375\n",
      "Evaluation for bmgb-2020-2.txt completed. Results saved.\n",
      "Processing file bpan-2019-4\n",
      "95/375\n",
      "Evaluation for bpan-2019-4.txt completed. Results saved.\n",
      "Processing file sanb-2017-4\n",
      "96/375\n",
      "Evaluation for sanb-2017-4.txt completed. Results saved.\n",
      "Processing file bbdc-2015-4\n",
      "97/375\n",
      "Evaluation for bbdc-2015-4.txt completed. Results saved.\n",
      "Processing file itub-2018-1\n",
      "98/375\n",
      "Evaluation for itub-2018-1.txt completed. Results saved.\n",
      "Processing file bbas-2020-2\n",
      "99/375\n",
      "Evaluation for bbas-2020-2.txt completed. Results saved.\n",
      "Processing file brsr-2021-1\n",
      "100/375\n",
      "Evaluation for brsr-2021-1.txt completed. Results saved.\n",
      "Processing file bbas-2012-4\n",
      "101/375\n",
      "Evaluation for bbas-2012-4.txt completed. Results saved.\n",
      "Processing file itub-2022-4\n",
      "102/375\n",
      "Evaluation for itub-2022-4.txt completed. Results saved.\n",
      "Processing file brsr-2009-3\n",
      "103/375\n",
      "Evaluation for brsr-2009-3.txt completed. Results saved.\n",
      "Processing file abcb-2008-2\n",
      "104/375\n",
      "Evaluation for abcb-2008-2.txt completed. Results saved.\n",
      "Processing file abcb-2016-1\n",
      "105/375\n",
      "Evaluation for abcb-2016-1.txt completed. Results saved.\n",
      "Processing file abcb-2016-4\n",
      "106/375\n",
      "Evaluation for abcb-2016-4.txt completed. Results saved.\n",
      "Processing file brsr-2020-3\n",
      "107/375\n",
      "Evaluation for brsr-2020-3.txt completed. Results saved.\n",
      "Processing file sanb-2016-4\n",
      "108/375\n",
      "Evaluation for sanb-2016-4.txt completed. Results saved.\n",
      "Processing file abcb-2014-4\n",
      "109/375\n",
      "Evaluation for abcb-2014-4.txt completed. Results saved.\n",
      "Processing file bmgb-2021-3\n",
      "110/375\n",
      "Evaluation for bmgb-2021-3.txt completed. Results saved.\n",
      "Processing file sanb-2018-3\n",
      "111/375\n",
      "Evaluation for sanb-2018-3.txt completed. Results saved.\n",
      "Processing file nu-2023-1\n",
      "112/375\n",
      "Evaluation for nu-2023-1.txt completed. Results saved.\n",
      "Processing file brsr-2015-1\n",
      "113/375\n",
      "Evaluation for brsr-2015-1.txt completed. Results saved.\n",
      "Processing file bbas-2015-1\n",
      "114/375\n",
      "Evaluation for bbas-2015-1.txt completed. Results saved.\n",
      "Processing file bbas-2023-2\n",
      "115/375\n",
      "Evaluation for bbas-2023-2.txt completed. Results saved.\n",
      "Processing file brsr-2009-1\n",
      "116/375\n",
      "Evaluation for brsr-2009-1.txt completed. Results saved.\n",
      "Processing file abcb-2023-1\n",
      "117/375\n",
      "Evaluation for abcb-2023-1.txt completed. Results saved.\n",
      "Processing file sanb-2016-3\n",
      "118/375\n",
      "Evaluation for sanb-2016-3.txt completed. Results saved.\n",
      "Processing file abcb-2014-2\n",
      "119/375\n",
      "Evaluation for abcb-2014-2.txt completed. Results saved.\n",
      "Processing file nu-2021-4\n",
      "120/375\n",
      "Evaluation for nu-2021-4.txt completed. Results saved.\n",
      "Processing file abcb-2018-4\n",
      "121/375\n",
      "Evaluation for abcb-2018-4.txt completed. Results saved.\n",
      "Processing file brsr-2010-4\n",
      "122/375\n",
      "Evaluation for brsr-2010-4.txt completed. Results saved.\n",
      "Processing file bbas-2016-2\n",
      "123/375\n",
      "Evaluation for bbas-2016-2.txt completed. Results saved.\n",
      "Processing file itub-2012-4\n",
      "124/375\n",
      "Evaluation for itub-2012-4.txt completed. Results saved.\n",
      "Processing file bbas-2021-1\n",
      "125/375\n",
      "Evaluation for bbas-2021-1.txt completed. Results saved.\n",
      "Processing file itub-2019-1\n",
      "126/375\n",
      "Evaluation for itub-2019-1.txt completed. Results saved.\n",
      "Processing file abcb-2019-4\n",
      "127/375\n",
      "Evaluation for abcb-2019-4.txt completed. Results saved.\n",
      "Processing file abcb-2010-3\n",
      "128/375\n",
      "Evaluation for abcb-2010-3.txt completed. Results saved.\n",
      "Processing file brsr-2011-1\n",
      "129/375\n",
      "Evaluation for brsr-2011-1.txt completed. Results saved.\n",
      "Processing file bpan-2016-1\n",
      "130/375\n",
      "Evaluation for bpan-2016-1.txt completed. Results saved.\n",
      "Processing file bmgb-2021-1\n",
      "131/375\n",
      "Evaluation for bmgb-2021-1.txt completed. Results saved.\n",
      "Processing file brsr-2011-2\n",
      "132/375\n",
      "Evaluation for brsr-2011-2.txt completed. Results saved.\n",
      "Processing file sanb-2013-2\n",
      "133/375\n",
      "Evaluation for sanb-2013-2.txt completed. Results saved.\n",
      "Processing file bbdc-2023-1\n",
      "134/375\n",
      "Evaluation for bbdc-2023-1.txt completed. Results saved.\n",
      "Processing file bbas-2008-1\n",
      "135/375\n",
      "Evaluation for bbas-2008-1.txt completed. Results saved.\n",
      "Processing file bbas-2017-3\n",
      "136/375\n",
      "Evaluation for bbas-2017-3.txt completed. Results saved.\n",
      "Processing file itub-2013-1\n",
      "137/375\n",
      "Evaluation for itub-2013-1.txt completed. Results saved.\n",
      "Processing file bbas-2019-3\n",
      "138/375\n",
      "Evaluation for bbas-2019-3.txt completed. Results saved.\n",
      "Processing file itub-2014-3\n",
      "139/375\n",
      "Error evaluating response: Expecting property name enclosed in double quotes: line 1 column 458 (char 457)\n",
      "Evaluation for itub-2014-3.txt completed. Results saved.\n",
      "Processing file brsr-2017-3\n",
      "140/375\n",
      "Evaluation for brsr-2017-3.txt completed. Results saved.\n",
      "Processing file bpan-2017-1\n",
      "141/375\n",
      "Evaluation for bpan-2017-1.txt completed. Results saved.\n",
      "Processing file brsr-2017-1\n",
      "142/375\n",
      "Evaluation for brsr-2017-1.txt completed. Results saved.\n",
      "Processing file bbas-2021-3\n",
      "143/375\n",
      "Evaluation for bbas-2021-3.txt completed. Results saved.\n",
      "Processing file itub-2021-1\n",
      "144/375\n",
      "Evaluation for itub-2021-1.txt completed. Results saved.\n",
      "Processing file bbdc-2020-1\n",
      "145/375\n",
      "Evaluation for bbdc-2020-1.txt completed. Results saved.\n",
      "Processing file brsr-2012-1\n",
      "146/375\n",
      "Evaluation for brsr-2012-1.txt completed. Results saved.\n",
      "Processing file sanb-2011-4\n",
      "147/375\n",
      "Evaluation for sanb-2011-4.txt completed. Results saved.\n",
      "Processing file bpan-2022-2\n",
      "148/375\n",
      "Evaluation for bpan-2022-2.txt completed. Results saved.\n",
      "Processing file sanb-2015-2\n",
      "149/375\n",
      "Error evaluating response: Invalid control character at: line 1 column 553 (char 552)\n",
      "Evaluation for sanb-2015-2.txt completed. Results saved.\n",
      "Processing file itub-2015-4\n",
      "150/375\n",
      "Evaluation for itub-2015-4.txt completed. Results saved.\n",
      "Processing file bbas-2009-3\n",
      "151/375\n",
      "Evaluation for bbas-2009-3.txt completed. Results saved.\n",
      "Processing file abcb-2009-3\n",
      "152/375\n",
      "Evaluation for abcb-2009-3.txt completed. Results saved.\n",
      "Processing file brsr-2009-2\n",
      "153/375\n",
      "Evaluation for brsr-2009-2.txt completed. Results saved.\n",
      "Processing file bbas-2012-3\n",
      "154/375\n",
      "Evaluation for bbas-2012-3.txt completed. Results saved.\n",
      "Processing file brsr-2016-4\n",
      "155/375\n",
      "Evaluation for brsr-2016-4.txt completed. Results saved.\n",
      "Processing file bpan-2019-3\n",
      "156/375\n",
      "Evaluation for bpan-2019-3.txt completed. Results saved.\n",
      "Processing file brsr-2020-1\n",
      "157/375\n",
      "Evaluation for brsr-2020-1.txt completed. Results saved.\n",
      "Processing file bbas-2014-4\n",
      "158/375\n",
      "Evaluation for bbas-2014-4.txt completed. Results saved.\n",
      "Processing file abcb-2009-4\n",
      "159/375\n",
      "Evaluation for abcb-2009-4.txt completed. Results saved.\n",
      "Processing file abcb-2019-2\n",
      "160/375\n",
      "Evaluation for abcb-2019-2.txt completed. Results saved.\n",
      "Processing file itub-2013-4\n",
      "161/375\n",
      "Evaluation for itub-2013-4.txt completed. Results saved.\n",
      "Processing file sanb-2013-4\n",
      "162/375\n",
      "Evaluation for sanb-2013-4.txt completed. Results saved.\n",
      "Processing file bpan-2016-2\n",
      "163/375\n",
      "Evaluation for bpan-2016-2.txt completed. Results saved.\n",
      "Processing file prbc-2011-2\n",
      "164/375\n",
      "Evaluation for prbc-2011-2.txt completed. Results saved.\n",
      "Processing file brsr-2018-1\n",
      "165/375\n",
      "Evaluation for brsr-2018-1.txt completed. Results saved.\n",
      "Processing file sanb-2012-4\n",
      "166/375\n",
      "Evaluation for sanb-2012-4.txt completed. Results saved.\n",
      "Processing file brsr-2014-1\n",
      "167/375\n",
      "Evaluation for brsr-2014-1.txt completed. Results saved.\n",
      "Processing file bbdc-2020-4\n",
      "168/375\n",
      "Evaluation for bbdc-2020-4.txt completed. Results saved.\n",
      "Processing file abcb-2020-2\n",
      "169/375\n",
      "Evaluation for abcb-2020-2.txt completed. Results saved.\n",
      "Processing file itub-2014-4\n",
      "170/375\n",
      "Evaluation for itub-2014-4.txt completed. Results saved.\n",
      "Processing file abcb-2022-2\n",
      "171/375\n",
      "Evaluation for abcb-2022-2.txt completed. Results saved.\n",
      "Processing file sanb-2014-4\n",
      "172/375\n",
      "Evaluation for sanb-2014-4.txt completed. Results saved.\n",
      "Processing file sanb-2012-3\n",
      "173/375\n",
      "Evaluation for sanb-2012-3.txt completed. Results saved.\n",
      "Processing file brsr-2012-3\n",
      "174/375\n",
      "Evaluation for brsr-2012-3.txt completed. Results saved.\n",
      "Processing file bbdc-2019-3\n",
      "175/375\n",
      "Evaluation for bbdc-2019-3.txt completed. Results saved.\n",
      "Processing file prbc-2011-4\n",
      "176/375\n",
      "Evaluation for prbc-2011-4.txt completed. Results saved.\n",
      "Processing file bbas-2008-3\n",
      "177/375\n",
      "Evaluation for bbas-2008-3.txt completed. Results saved.\n",
      "Processing file bbas-2015-3\n",
      "178/375\n",
      "Evaluation for bbas-2015-3.txt completed. Results saved.\n",
      "Processing file bbas-2017-1\n",
      "179/375\n",
      "Evaluation for bbas-2017-1.txt completed. Results saved.\n",
      "Processing file bbas-2012-2\n",
      "180/375\n",
      "Evaluation for bbas-2012-2.txt completed. Results saved.\n",
      "Processing file bbdc-2017-1\n",
      "181/375\n",
      "Evaluation for bbdc-2017-1.txt completed. Results saved.\n",
      "Processing file brsr-2022-3\n",
      "182/375\n",
      "Evaluation for brsr-2022-3.txt completed. Results saved.\n",
      "Processing file itub-2014-1\n",
      "183/375\n",
      "Evaluation for itub-2014-1.txt completed. Results saved.\n",
      "Processing file itub-2011-1\n",
      "184/375\n",
      "Evaluation for itub-2011-1.txt completed. Results saved.\n",
      "Processing file bbdc-2015-1\n",
      "185/375\n",
      "Evaluation for bbdc-2015-1.txt completed. Results saved.\n",
      "Processing file itub-2015-2\n",
      "186/375\n",
      "Evaluation for itub-2015-2.txt completed. Results saved.\n",
      "Processing file sanb-2012-2\n",
      "187/375\n",
      "Evaluation for sanb-2012-2.txt completed. Results saved.\n",
      "Processing file bbas-2022-3\n",
      "188/375\n",
      "Evaluation for bbas-2022-3.txt completed. Results saved.\n",
      "Processing file bmgb-2019-1\n",
      "189/375\n",
      "Evaluation for bmgb-2019-1.txt completed. Results saved.\n",
      "Processing file bbas-2010-2\n",
      "190/375\n",
      "Evaluation for bbas-2010-2.txt completed. Results saved.\n",
      "Processing file bpan-2022-4\n",
      "191/375\n",
      "Evaluation for bpan-2022-4.txt completed. Results saved.\n",
      "Processing file itub-2019-2\n",
      "192/375\n",
      "Evaluation for itub-2019-2.txt completed. Results saved.\n",
      "Processing file brsr-2021-2\n",
      "193/375\n",
      "Evaluation for brsr-2021-2.txt completed. Results saved.\n",
      "Processing file abcb-2015-3\n",
      "194/375\n",
      "Evaluation for abcb-2015-3.txt completed. Results saved.\n",
      "Processing file sanb-2020-1\n",
      "195/375\n",
      "Evaluation for sanb-2020-1.txt completed. Results saved.\n",
      "Processing file sanb-2019-2\n",
      "196/375\n",
      "Evaluation for sanb-2019-2.txt completed. Results saved.\n",
      "Processing file abcb-2011-1\n",
      "197/375\n",
      "Evaluation for abcb-2011-1.txt completed. Results saved.\n",
      "Processing file abcb-2012-1\n",
      "198/375\n",
      "Evaluation for abcb-2012-1.txt completed. Results saved.\n",
      "Processing file bbdc-2020-2\n",
      "199/375\n",
      "Evaluation for bbdc-2020-2.txt completed. Results saved.\n",
      "Processing file abcb-2017-4\n",
      "200/375\n",
      "Evaluation for abcb-2017-4.txt completed. Results saved.\n",
      "Processing file brsr-2019-4\n",
      "201/375\n",
      "Evaluation for brsr-2019-4.txt completed. Results saved.\n",
      "Processing file bbdc-2020-3\n",
      "202/375\n",
      "Evaluation for bbdc-2020-3.txt completed. Results saved.\n",
      "Processing file prbc-2013-2\n",
      "203/375\n",
      "Evaluation for prbc-2013-2.txt completed. Results saved.\n",
      "Processing file prbc-2012-2\n",
      "204/375\n",
      "Evaluation for prbc-2012-2.txt completed. Results saved.\n",
      "Processing file itub-2018-3\n",
      "205/375\n",
      "Evaluation for itub-2018-3.txt completed. Results saved.\n",
      "Processing file bbdc-2015-2\n",
      "206/375\n",
      "Evaluation for bbdc-2015-2.txt completed. Results saved.\n",
      "Processing file brsr-2008-4\n",
      "207/375\n",
      "Evaluation for brsr-2008-4.txt completed. Results saved.\n",
      "Processing file sanb-2021-1\n",
      "208/375\n",
      "Evaluation for sanb-2021-1.txt completed. Results saved.\n",
      "Processing file itub-2020-3\n",
      "209/375\n",
      "Evaluation for itub-2020-3.txt completed. Results saved.\n",
      "Processing file bpan-2016-3\n",
      "210/375\n",
      "Evaluation for bpan-2016-3.txt completed. Results saved.\n",
      "Processing file itub-2012-3\n",
      "211/375\n",
      "Evaluation for itub-2012-3.txt completed. Results saved.\n",
      "Processing file bbas-2016-4\n",
      "212/375\n",
      "Evaluation for bbas-2016-4.txt completed. Results saved.\n",
      "Processing file bbdc-2023-2\n",
      "213/375\n",
      "Evaluation for bbdc-2023-2.txt completed. Results saved.\n",
      "Processing file abcb-2021-2\n",
      "214/375\n",
      "Evaluation for abcb-2021-2.txt completed. Results saved.\n",
      "Processing file bmgb-2022-1\n",
      "215/375\n",
      "Evaluation for bmgb-2022-1.txt completed. Results saved.\n",
      "Processing file bbdc-2021-2\n",
      "216/375\n",
      "Evaluation for bbdc-2021-2.txt completed. Results saved.\n",
      "Processing file bbas-2008-2\n",
      "217/375\n",
      "Evaluation for bbas-2008-2.txt completed. Results saved.\n",
      "Processing file itub-2013-3\n",
      "218/375\n",
      "Evaluation for itub-2013-3.txt completed. Results saved.\n",
      "Processing file bbas-2007-3\n",
      "219/375\n",
      "Evaluation for bbas-2007-3.txt completed. Results saved.\n",
      "Processing file bpan-2020-2\n",
      "220/375\n",
      "Evaluation for bpan-2020-2.txt completed. Results saved.\n",
      "Processing file sanb-2021-2\n",
      "221/375\n",
      "Evaluation for sanb-2021-2.txt completed. Results saved.\n",
      "Processing file abcb-2008-1\n",
      "222/375\n",
      "Evaluation for abcb-2008-1.txt completed. Results saved.\n",
      "Processing file bbas-2012-1\n",
      "223/375\n",
      "Evaluation for bbas-2012-1.txt completed. Results saved.\n",
      "Processing file brsr-2013-3\n",
      "224/375\n",
      "Evaluation for brsr-2013-3.txt completed. Results saved.\n",
      "Processing file bbdc-2018-4\n",
      "225/375\n",
      "Evaluation for bbdc-2018-4.txt completed. Results saved.\n",
      "Processing file sanb-2021-4\n",
      "226/375\n",
      "Evaluation for sanb-2021-4.txt completed. Results saved.\n",
      "Processing file prbc-2012-3\n",
      "227/375\n",
      "Evaluation for prbc-2012-3.txt completed. Results saved.\n",
      "Processing file itub-2021-2\n",
      "228/375\n",
      "Evaluation for itub-2021-2.txt completed. Results saved.\n",
      "Processing file brsr-2022-2\n",
      "229/375\n",
      "Evaluation for brsr-2022-2.txt completed. Results saved.\n",
      "Processing file brsr-2016-1\n",
      "230/375\n",
      "Evaluation for brsr-2016-1.txt completed. Results saved.\n",
      "Processing file bbas-2021-4\n",
      "231/375\n",
      "Evaluation for bbas-2021-4.txt completed. Results saved.\n",
      "Processing file bbas-2018-3\n",
      "232/375\n",
      "Evaluation for bbas-2018-3.txt completed. Results saved.\n",
      "Processing file sanb-2010-4\n",
      "233/375\n",
      "Evaluation for sanb-2010-4.txt completed. Results saved.\n",
      "Processing file abcb-2017-2\n",
      "234/375\n",
      "Evaluation for abcb-2017-2.txt completed. Results saved.\n",
      "Processing file brsr-2009-4\n",
      "235/375\n",
      "Evaluation for brsr-2009-4.txt completed. Results saved.\n",
      "Processing file bbas-2014-2\n",
      "236/375\n",
      "Evaluation for bbas-2014-2.txt completed. Results saved.\n",
      "Processing file sanb-2010-1\n",
      "237/375\n",
      "Evaluation for sanb-2010-1.txt completed. Results saved.\n",
      "Processing file sanb-2018-4\n",
      "238/375\n",
      "Evaluation for sanb-2018-4.txt completed. Results saved.\n",
      "Processing file brsr-2010-2\n",
      "239/375\n",
      "Evaluation for brsr-2010-2.txt completed. Results saved.\n",
      "Processing file bpan-2021-3\n",
      "240/375\n",
      "Evaluation for bpan-2021-3.txt completed. Results saved.\n",
      "Processing file bpan-2023-1\n",
      "241/375\n",
      "Evaluation for bpan-2023-1.txt completed. Results saved.\n",
      "Processing file bbas-2011-2\n",
      "242/375\n",
      "Evaluation for bbas-2011-2.txt completed. Results saved.\n",
      "Processing file brsr-2015-3\n",
      "243/375\n",
      "Evaluation for brsr-2015-3.txt completed. Results saved.\n",
      "Processing file bbas-2016-1\n",
      "244/375\n",
      "Evaluation for bbas-2016-1.txt completed. Results saved.\n",
      "Processing file itub-2019-4\n",
      "245/375\n",
      "Evaluation for itub-2019-4.txt completed. Results saved.\n",
      "Processing file sanb-2011-1\n",
      "246/375\n",
      "Evaluation for sanb-2011-1.txt completed. Results saved.\n",
      "Processing file brsr-2015-2\n",
      "247/375\n",
      "Evaluation for brsr-2015-2.txt completed. Results saved.\n",
      "Processing file bbas-2015-4\n",
      "248/375\n",
      "Evaluation for bbas-2015-4.txt completed. Results saved.\n",
      "Processing file bbas-2014-3\n",
      "249/375\n",
      "Evaluation for bbas-2014-3.txt completed. Results saved.\n",
      "Processing file bbas-2021-2\n",
      "250/375\n",
      "Evaluation for bbas-2021-2.txt completed. Results saved.\n",
      "Processing file bbdc-2016-1\n",
      "251/375\n",
      "Evaluation for bbdc-2016-1.txt completed. Results saved.\n",
      "Processing file sanb-2016-1\n",
      "252/375\n",
      "Evaluation for sanb-2016-1.txt completed. Results saved.\n",
      "Processing file sanb-2018-1\n",
      "253/375\n",
      "Evaluation for sanb-2018-1.txt completed. Results saved.\n",
      "Processing file prbc-2009-4\n",
      "254/375\n",
      "Evaluation for prbc-2009-4.txt completed. Results saved.\n",
      "Processing file bbas-2022-1\n",
      "255/375\n",
      "Evaluation for bbas-2022-1.txt completed. Results saved.\n",
      "Processing file bbas-2008-4\n",
      "256/375\n",
      "Evaluation for bbas-2008-4.txt completed. Results saved.\n",
      "Processing file itub-2011-3\n",
      "257/375\n",
      "Evaluation for itub-2011-3.txt completed. Results saved.\n",
      "Processing file itub-2018-2\n",
      "258/375\n",
      "Evaluation for itub-2018-2.txt completed. Results saved.\n",
      "Processing file abcb-2019-1\n",
      "259/375\n",
      "Evaluation for abcb-2019-1.txt completed. Results saved.\n",
      "Processing file abcb-2020-3\n",
      "260/375\n",
      "Evaluation for abcb-2020-3.txt completed. Results saved.\n",
      "Processing file bbas-2020-4\n",
      "261/375\n",
      "Evaluation for bbas-2020-4.txt completed. Results saved.\n",
      "Processing file brsr-2013-1\n",
      "262/375\n",
      "Evaluation for brsr-2013-1.txt completed. Results saved.\n",
      "Processing file brsr-2022-4\n",
      "263/375\n",
      "Evaluation for brsr-2022-4.txt completed. Results saved.\n",
      "Processing file abcb-2021-4\n",
      "264/375\n",
      "Evaluation for abcb-2021-4.txt completed. Results saved.\n",
      "Processing file bbdc-2018-3\n",
      "265/375\n",
      "Evaluation for bbdc-2018-3.txt completed. Results saved.\n",
      "Processing file bbas-2010-1\n",
      "266/375\n",
      "Evaluation for bbas-2010-1.txt completed. Results saved.\n",
      "Processing file abcb-2013-4\n",
      "267/375\n",
      "Evaluation for abcb-2013-4.txt completed. Results saved.\n",
      "Processing file itub-2022-3\n",
      "268/375\n",
      "Evaluation for itub-2022-3.txt completed. Results saved.\n",
      "Processing file bbdc-2022-4\n",
      "269/375\n",
      "Evaluation for bbdc-2022-4.txt completed. Results saved.\n",
      "Processing file bpan-2022-1\n",
      "270/375\n",
      "Evaluation for bpan-2022-1.txt completed. Results saved.\n",
      "Processing file brsr-2008-2\n",
      "271/375\n",
      "Evaluation for brsr-2008-2.txt completed. Results saved.\n",
      "Processing file abcb-2021-1\n",
      "272/375\n",
      "Evaluation for abcb-2021-1.txt completed. Results saved.\n",
      "Processing file abcb-2009-2\n",
      "273/375\n",
      "Evaluation for abcb-2009-2.txt completed. Results saved.\n",
      "Processing file bbdc-2016-2\n",
      "274/375\n",
      "Evaluation for bbdc-2016-2.txt completed. Results saved.\n",
      "Processing file abcb-2018-2\n",
      "275/375\n",
      "Evaluation for abcb-2018-2.txt completed. Results saved.\n",
      "Processing file abcb-2023-2\n",
      "276/375\n",
      "Evaluation for abcb-2023-2.txt completed. Results saved.\n",
      "Processing file abcb-2015-1\n",
      "277/375\n",
      "Evaluation for abcb-2015-1.txt completed. Results saved.\n",
      "Processing file bbas-2015-2\n",
      "278/375\n",
      "Evaluation for bbas-2015-2.txt completed. Results saved.\n",
      "Processing file itub-2016-4\n",
      "279/375\n",
      "Evaluation for itub-2016-4.txt completed. Results saved.\n",
      "Processing file brsr-2018-2\n",
      "280/375\n",
      "Evaluation for brsr-2018-2.txt completed. Results saved.\n",
      "Processing file sanb-2019-4\n",
      "281/375\n",
      "Evaluation for sanb-2019-4.txt completed. Results saved.\n",
      "Processing file bmgb-2021-4\n",
      "282/375\n",
      "Evaluation for bmgb-2021-4.txt completed. Results saved.\n",
      "Processing file sanb-2012-1\n",
      "283/375\n",
      "Evaluation for sanb-2012-1.txt completed. Results saved.\n",
      "Processing file itub-2017-4\n",
      "284/375\n",
      "Evaluation for itub-2017-4.txt completed. Results saved.\n",
      "Processing file sanb-2018-2\n",
      "285/375\n",
      "Evaluation for sanb-2018-2.txt completed. Results saved.\n",
      "Processing file abcb-2015-2\n",
      "286/375\n",
      "Evaluation for abcb-2015-2.txt completed. Results saved.\n",
      "Processing file bbas-2020-3\n",
      "287/375\n",
      "Evaluation for bbas-2020-3.txt completed. Results saved.\n",
      "Processing file prbc-2010-3\n",
      "288/375\n",
      "Evaluation for prbc-2010-3.txt completed. Results saved.\n",
      "Processing file brsr-2012-4\n",
      "289/375\n",
      "Evaluation for brsr-2012-4.txt completed. Results saved.\n",
      "Processing file bmgb-2020-1\n",
      "290/375\n",
      "Evaluation for bmgb-2020-1.txt completed. Results saved.\n",
      "Processing file bmgb-2022-3\n",
      "291/375\n",
      "Evaluation for bmgb-2022-3.txt completed. Results saved.\n",
      "Processing file bpan-2017-3\n",
      "292/375\n",
      "Error evaluating response: Expecting ',' delimiter: line 1 column 745 (char 744)\n",
      "Evaluation for bpan-2017-3.txt completed. Results saved.\n",
      "Processing file bmgb-2020-4\n",
      "293/375\n",
      "Evaluation for bmgb-2020-4.txt completed. Results saved.\n",
      "Processing file bbdc-2017-2\n",
      "294/375\n",
      "Evaluation for bbdc-2017-2.txt completed. Results saved.\n",
      "Processing file itub-2023-1\n",
      "295/375\n",
      "Evaluation for itub-2023-1.txt completed. Results saved.\n",
      "Processing file bbas-2022-2\n",
      "296/375\n",
      "Evaluation for bbas-2022-2.txt completed. Results saved.\n",
      "Processing file brsr-2020-4\n",
      "297/375\n",
      "Evaluation for brsr-2020-4.txt completed. Results saved.\n",
      "Processing file bbdc-2016-4\n",
      "298/375\n",
      "Evaluation for bbdc-2016-4.txt completed. Results saved.\n",
      "Processing file bbas-2011-1\n",
      "299/375\n",
      "Evaluation for bbas-2011-1.txt completed. Results saved.\n",
      "Processing file itub-2019-3\n",
      "300/375\n",
      "Evaluation for itub-2019-3.txt completed. Results saved.\n",
      "Processing file itub-2012-1\n",
      "301/375\n",
      "Evaluation for itub-2012-1.txt completed. Results saved.\n",
      "Processing file bbdc-2018-2\n",
      "302/375\n",
      "Evaluation for bbdc-2018-2.txt completed. Results saved.\n",
      "Processing file bbdc-2016-3\n",
      "303/375\n",
      "Evaluation for bbdc-2016-3.txt completed. Results saved.\n",
      "Processing file bbas-2013-2\n",
      "304/375\n",
      "Evaluation for bbas-2013-2.txt completed. Results saved.\n",
      "Processing file itub-2016-3\n",
      "305/375\n",
      "Evaluation for itub-2016-3.txt completed. Results saved.\n",
      "Processing file abcb-2019-3\n",
      "306/375\n",
      "Evaluation for abcb-2019-3.txt completed. Results saved.\n",
      "Processing file brsr-2020-2\n",
      "307/375\n",
      "Evaluation for brsr-2020-2.txt completed. Results saved.\n",
      "Processing file sanb-2017-3\n",
      "308/375\n",
      "Evaluation for sanb-2017-3.txt completed. Results saved.\n",
      "Processing file abcb-2012-2\n",
      "309/375\n",
      "Evaluation for abcb-2012-2.txt completed. Results saved.\n",
      "Processing file sanb-2017-2\n",
      "310/375\n",
      "Evaluation for sanb-2017-2.txt completed. Results saved.\n",
      "Processing file bbdc-2021-4\n",
      "311/375\n",
      "Evaluation for bbdc-2021-4.txt completed. Results saved.\n",
      "Processing file sanb-2014-2\n",
      "312/375\n",
      "Evaluation for sanb-2014-2.txt completed. Results saved.\n",
      "Processing file bbdc-2022-3\n",
      "313/375\n",
      "Evaluation for bbdc-2022-3.txt completed. Results saved.\n",
      "Processing file itub-2021-4\n",
      "314/375\n",
      "Evaluation for itub-2021-4.txt completed. Results saved.\n",
      "Processing file sanb-2017-1\n",
      "315/375\n",
      "Evaluation for sanb-2017-1.txt completed. Results saved.\n",
      "Processing file brsr-2021-4\n",
      "316/375\n",
      "Evaluation for brsr-2021-4.txt completed. Results saved.\n",
      "Processing file brsr-2023-2\n",
      "317/375\n",
      "Evaluation for brsr-2023-2.txt completed. Results saved.\n",
      "Processing file bpan-2012-2\n",
      "318/375\n",
      "Evaluation for bpan-2012-2.txt completed. Results saved.\n",
      "Processing file itub-2022-1\n",
      "319/375\n",
      "Evaluation for itub-2022-1.txt completed. Results saved.\n",
      "Processing file sanb-2015-4\n",
      "320/375\n",
      "Evaluation for sanb-2015-4.txt completed. Results saved.\n",
      "Processing file bbas-2019-1\n",
      "321/375\n",
      "Evaluation for bbas-2019-1.txt completed. Results saved.\n",
      "Processing file bbas-2016-3\n",
      "322/375\n",
      "Evaluation for bbas-2016-3.txt completed. Results saved.\n",
      "Processing file brsr-2016-3\n",
      "323/375\n",
      "Evaluation for brsr-2016-3.txt completed. Results saved.\n",
      "Processing file bbas-2023-1\n",
      "324/375\n",
      "Evaluation for bbas-2023-1.txt completed. Results saved.\n",
      "Processing file bbas-2009-4\n",
      "325/375\n",
      "Evaluation for bbas-2009-4.txt completed. Results saved.\n",
      "Processing file abcb-2022-1\n",
      "326/375\n",
      "Evaluation for abcb-2022-1.txt completed. Results saved.\n",
      "Processing file itub-2020-4\n",
      "327/375\n",
      "Evaluation for itub-2020-4.txt completed. Results saved.\n",
      "Processing file brsr-2019-2\n",
      "328/375\n",
      "Evaluation for brsr-2019-2.txt completed. Results saved.\n",
      "Processing file prbc-2009-1\n",
      "329/375\n",
      "Evaluation for prbc-2009-1.txt completed. Results saved.\n",
      "Processing file nu-2022-1\n",
      "330/375\n",
      "Evaluation for nu-2022-1.txt completed. Results saved.\n",
      "Processing file abcb-2012-4\n",
      "331/375\n",
      "Evaluation for abcb-2012-4.txt completed. Results saved.\n",
      "Processing file bbas-2009-2\n",
      "332/375\n",
      "Evaluation for bbas-2009-2.txt completed. Results saved.\n",
      "Processing file bpan-2012-3\n",
      "333/375\n",
      "Evaluation for bpan-2012-3.txt completed. Results saved.\n",
      "Processing file bbas-2013-4\n",
      "334/375\n",
      "Evaluation for bbas-2013-4.txt completed. Results saved.\n",
      "Processing file abcb-2010-1\n",
      "335/375\n",
      "Evaluation for abcb-2010-1.txt completed. Results saved.\n",
      "Processing file bbas-2010-3\n",
      "336/375\n",
      "Evaluation for bbas-2010-3.txt completed. Results saved.\n",
      "Processing file abcb-2021-3\n",
      "337/375\n",
      "Evaluation for abcb-2021-3.txt completed. Results saved.\n",
      "Processing file sanb-2023-1\n",
      "338/375\n",
      "Evaluation for sanb-2023-1.txt completed. Results saved.\n",
      "Processing file bbas-2017-2\n",
      "339/375\n",
      "Evaluation for bbas-2017-2.txt completed. Results saved.\n",
      "Processing file sanb-2010-2\n",
      "340/375\n",
      "Evaluation for sanb-2010-2.txt completed. Results saved.\n",
      "Processing file abcb-2012-3\n",
      "341/375\n",
      "Evaluation for abcb-2012-3.txt completed. Results saved.\n",
      "Processing file sanb-2013-1\n",
      "342/375\n",
      "Evaluation for sanb-2013-1.txt completed. Results saved.\n",
      "Processing file abcb-2013-2\n",
      "343/375\n",
      "Evaluation for abcb-2013-2.txt completed. Results saved.\n",
      "Processing file bmgb-2022-4\n",
      "344/375\n",
      "Evaluation for bmgb-2022-4.txt completed. Results saved.\n",
      "Processing file sanb-2022-4\n",
      "345/375\n",
      "Evaluation for sanb-2022-4.txt completed. Results saved.\n",
      "Processing file bbas-2013-3\n",
      "346/375\n",
      "Evaluation for bbas-2013-3.txt completed. Results saved.\n",
      "Processing file itub-2015-3\n",
      "347/375\n",
      "Evaluation for itub-2015-3.txt completed. Results saved.\n",
      "Processing file itub-2014-2\n",
      "348/375\n",
      "Evaluation for itub-2014-2.txt completed. Results saved.\n",
      "Processing file abcb-2018-3\n",
      "349/375\n",
      "Evaluation for abcb-2018-3.txt completed. Results saved.\n",
      "Processing file abcb-2017-3\n",
      "350/375\n",
      "Evaluation for abcb-2017-3.txt completed. Results saved.\n",
      "Processing file bpan-2019-1\n",
      "351/375\n",
      "Evaluation for bpan-2019-1.txt completed. Results saved.\n",
      "Processing file prbc-2011-1\n",
      "352/375\n",
      "Evaluation for prbc-2011-1.txt completed. Results saved.\n",
      "Processing file abcb-2011-4\n",
      "353/375\n",
      "Evaluation for abcb-2011-4.txt completed. Results saved.\n",
      "Processing file bbas-2018-1\n",
      "354/375\n",
      "Evaluation for bbas-2018-1.txt completed. Results saved.\n",
      "Processing file brsr-2008-1\n",
      "355/375\n",
      "Evaluation for brsr-2008-1.txt completed. Results saved.\n",
      "Processing file brsr-2013-2\n",
      "356/375\n",
      "Evaluation for brsr-2013-2.txt completed. Results saved.\n",
      "Processing file abcb-2020-1\n",
      "357/375\n",
      "Evaluation for abcb-2020-1.txt completed. Results saved.\n",
      "Processing file bbdc-2015-3\n",
      "358/375\n",
      "Evaluation for bbdc-2015-3.txt completed. Results saved.\n",
      "Processing file itub-2011-4\n",
      "359/375\n",
      "Evaluation for itub-2011-4.txt completed. Results saved.\n",
      "Processing file sanb-2020-3\n",
      "360/375\n",
      "Evaluation for sanb-2020-3.txt completed. Results saved.\n",
      "Processing file brsr-2014-2\n",
      "361/375\n",
      "Evaluation for brsr-2014-2.txt completed. Results saved.\n",
      "Processing file abcb-2017-1\n",
      "362/375\n",
      "Evaluation for abcb-2017-1.txt completed. Results saved.\n",
      "Processing file bbdc-2017-4\n",
      "363/375\n",
      "Evaluation for bbdc-2017-4.txt completed. Results saved.\n",
      "Processing file bbdc-2021-3\n",
      "364/375\n",
      "Evaluation for bbdc-2021-3.txt completed. Results saved.\n",
      "Processing file bbas-2018-4\n",
      "365/375\n",
      "Evaluation for bbas-2018-4.txt completed. Results saved.\n",
      "Processing file sanb-2022-3\n",
      "366/375\n",
      "Evaluation for sanb-2022-3.txt completed. Results saved.\n",
      "Processing file bbas-2022-4\n",
      "367/375\n",
      "Evaluation for bbas-2022-4.txt completed. Results saved.\n",
      "Processing file sanb-2023-2\n",
      "368/375\n",
      "Evaluation for sanb-2023-2.txt completed. Results saved.\n",
      "Processing file itub-2022-2\n",
      "369/375\n",
      "Evaluation for itub-2022-2.txt completed. Results saved.\n",
      "Processing file bbas-2019-4\n",
      "370/375\n",
      "Evaluation for bbas-2019-4.txt completed. Results saved.\n",
      "Processing file itub-2012-2\n",
      "371/375\n",
      "Evaluation for itub-2012-2.txt completed. Results saved.\n",
      "Processing file bbas-2014-1\n",
      "372/375\n",
      "Evaluation for bbas-2014-1.txt completed. Results saved.\n",
      "Processing file brsr-2016-2\n",
      "373/375\n",
      "Evaluation for brsr-2016-2.txt completed. Results saved.\n",
      "Processing file bmgb-2020-3\n",
      "374/375\n",
      "Evaluation for bmgb-2020-3.txt completed. Results saved.\n",
      "Processing file bmgb-2021-2\n",
      "375/375\n",
      "Evaluation for bmgb-2021-2.txt completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "# Set up your OpenAI API key\n",
    "api_key = \"example_key\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the evaluation prompt\n",
    "evaluation_prompt = \"\"\"\n",
    "Você deve avaliar a resposta de um modelo para a tarefa 1 demandada e fornecer uma pontuação de 0 a 10, junto com uma explicação para a pontuação.\n",
    "Considere:\n",
    "\n",
    "1. A aderência ao pedido no prompt original.\n",
    "2. Os temas serem os mais relevantes.\n",
    "3. A aderência ao formato solicitado. Seja em escrita e quantidade de tópicos.\n",
    "\n",
    "Tarefa original:\n",
    "{original_prompt}\n",
    "\n",
    "Texto original:\n",
    "{context}\n",
    "\n",
    "Resposta do Modelo:\n",
    "{response}\n",
    "\n",
    "Qual é a pontuação (0 a 10) e a explicação? Forneça no formato:\n",
    "{{\"score\": <pontuação>, \"explanation\": \"<explicação>\"}}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_response_with_gpt(original_prompt, context, response, model=\"gpt-4-turbo\"):\n",
    "    \"\"\"Evaluate a single response using the ChatGPT API.\"\"\"\n",
    "    try:\n",
    "        evaluation_question = evaluation_prompt.format(\n",
    "            original_prompt=original_prompt,\n",
    "            context=context,\n",
    "            response=response\n",
    "        )\n",
    "        api_response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um avaliador expert\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_question}\n",
    "            ]\n",
    "        )\n",
    "        evaluation_result = api_response['choices'][0]['message']['content'].strip()\n",
    "        result = json.loads(evaluation_result)  # Validate JSON format\n",
    "        if \"score\" in result and \"explanation\" in result:\n",
    "            return result\n",
    "        else:\n",
    "            return {\"score\": None, \"explanation\": \"Invalid response format\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating response: {e}\")\n",
    "        return {\"score\": None, \"explanation\": \"Error during evaluation\"}\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Read and return the contents of a file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Original prompt that was given to the models\n",
    "original_prompt = \"\"\"\n",
    "      Queria pedir para você realizar duas tarefas sequencialmente:\n",
    "\n",
    "      Tarefa 1) Apresentar os tópicos mais importantes desse texto. Limite máximo de 10 tópicos. Os tópicos devem ser de no máximo 5 palavras e devem ser assuntos, não o detalhamento do que foi falado. Liste os tópicos de em tópicos com '-'.\n",
    "      Tarefa 2) Avaliar pelas perguntas do público se o público teve uma percepção positiva do apresentado. A resposta deve ter 1 palavra: positivo ou negativo.\n",
    "\n",
    "      Para todas as respostas deve-se começar pelo texto: 'Tarefa x:' e usar tópicos usando '-'\n",
    "      Não deve-se usar *\n",
    "    \"\"\"\n",
    "\n",
    "# Paths in Colab (use mounted Google Drive or local paths)\n",
    "original_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Divided_text/qna/\"\n",
    "qwen_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/qwen/unsupervised/\"\n",
    "llama_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/llama/unsupervised/\"\n",
    "output_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/contextualized/\"\n",
    "error_file = os.path.join(output_folder, \"error.txt\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Example workflow: Processing all files\n",
    "files = [f for f in os.listdir(original_folder) if f.endswith('.txt')]\n",
    "#files = files[:5]\n",
    "\n",
    "num_files = len(files)\n",
    "cont = 0\n",
    "for filename in files:\n",
    "    name_file = filename.split(\".\")[0]\n",
    "    print(f\"Processing file {name_file}\")\n",
    "    cont += 1\n",
    "    print(f\"{cont}/{num_files}\")\n",
    "\n",
    "    try:\n",
    "        # Read the original text and model responses\n",
    "        original_text = read_file(os.path.join(original_folder, filename))\n",
    "        qwen_response = read_file(os.path.join(qwen_folder, f\"{name_file}.txt_output.txt\"))\n",
    "        llama_response = read_file(os.path.join(llama_folder, f\"{name_file}.txt_output.txt\"))\n",
    "\n",
    "        # Evaluate responses\n",
    "        qwen_result = evaluate_response_with_gpt(original_prompt, original_text, qwen_response)\n",
    "        llama_result = evaluate_response_with_gpt(original_prompt, original_text, llama_response)\n",
    "\n",
    "        # Save results\n",
    "        evaluation_data = {\n",
    "            \"llama\": llama_result,\n",
    "            \"qwen\": qwen_result\n",
    "        }\n",
    "        output_file = os.path.join(output_folder, f\"{name_file}_evaluation.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(evaluation_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Evaluation for {filename} completed. Results saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log errors\n",
    "        with open(error_file, 'a') as ef:\n",
    "            ef.write(f\"{filename}\\n\")\n",
    "        print(f\"Evaluation for {filename} failed. Error logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9IW4wVXxg0"
   },
   "source": [
    "# Generate model-level assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaiFg1t7igd"
   },
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhRfZjp89tPv"
   },
   "outputs": [],
   "source": [
    "api_key = \"example_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19878,
     "status": "ok",
     "timestamp": 1736073792284,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "Ju84CcVr7j3Y",
    "outputId": "d5a77aa3-3997-44f5-e723-9ef4d5c4e6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: llama\n",
      "No data available for model: llama.\n",
      "Processing model: qwen\n",
      "No data available for model: qwen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def aggregate_model_evaluations(model_folder):\n",
    "    \"\"\"Aggregate evaluations for a specific model.\"\"\"\n",
    "    aggregated_explanations = []\n",
    "    aggregated_scores = []\n",
    "\n",
    "    # Collect all evaluation files for the model\n",
    "    evaluation_files = [f for f in os.listdir(model_folder) if f.endswith('_evaluation.json')]\n",
    "\n",
    "    for eval_file in evaluation_files:\n",
    "        with open(os.path.join(model_folder, eval_file), 'r') as f:\n",
    "            eval_data = json.load(f)\n",
    "            if \"score\" in eval_data and eval_data[\"score\"] is not None:\n",
    "                aggregated_scores.append(eval_data[\"score\"])\n",
    "                aggregated_explanations.append(eval_data[\"explanation\"])\n",
    "\n",
    "    return aggregated_scores, aggregated_explanations\n",
    "\n",
    "\n",
    "def generate_full_assessment(aggregated_scores, aggregated_explanations, model_name, api_key):\n",
    "    \"\"\"Generate a full model assessment using the ChatGPT API.\"\"\"\n",
    "    # Prepare aggregated data\n",
    "    explanations_text = \"\\n\\n\".join(aggregated_explanations)\n",
    "    average_score = sum(aggregated_scores) / len(aggregated_scores)\n",
    "\n",
    "    # Create evaluation prompt\n",
    "    assessment_prompt = f\"\"\"\n",
    "    Você deve fornecer uma avaliação geral para o modelo '{model_name}' com base nos dados a seguir:\n",
    "\n",
    "    - Pontuação média: {average_score:.2f}\n",
    "    - Explicações agregadas:\n",
    "    {explanations_text}\n",
    "\n",
    "    Avalie os seguintes aspectos:\n",
    "    1. Os pontos mais fortes do modelo.\n",
    "    2. Os pontos mais fracos do modelo.\n",
    "    3. Recomendações para melhoria.\n",
    "    4. Um resumo geral da performance.\n",
    "\n",
    "    Forneça a resposta no seguinte formato:\n",
    "    {{\n",
    "        \"strengths\": [\"<forte1>\", \"<forte2>\", ...],\n",
    "        \"weaknesses\": [\"<fraqueza1>\", \"<fraqueza2>\", ...],\n",
    "        \"recommendations\": [\"<recomendação1>\", \"<recomendação2>\", ...],\n",
    "        \"summary\": \"<resumo>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call ChatGPT API\n",
    "    try:\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "        api_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um avaliador expert em modelos de linguagem.\"},\n",
    "                {\"role\": \"user\", \"content\": assessment_prompt}\n",
    "            ]\n",
    "        )\n",
    "        assessment_result = api_response['choices'][0]['message']['content'].strip()\n",
    "        return json.loads(assessment_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating assessment for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main Workflow\n",
    "api_key = \"example_key\"\n",
    "models_folders = {\n",
    "    \"llama\": \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_llama/contextualized/\",\n",
    "    \"qwen\":  \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_qwen/contextualized/\"\n",
    "}\n",
    "output_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/full_model_assessments/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each model\n",
    "for model_name, model_folder in models_folders.items():\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    # Aggregate evaluations for the model\n",
    "    aggregated_scores, aggregated_explanations = aggregate_model_evaluations(model_folder)\n",
    "\n",
    "    # Generate the full assessment\n",
    "    if aggregated_scores and aggregated_explanations:\n",
    "        final_assessment = generate_full_assessment(aggregated_scores, aggregated_explanations, model_name, api_key)\n",
    "\n",
    "        # Save the assessment to a separate file\n",
    "        if final_assessment:\n",
    "            assessment_file = os.path.join(output_folder, f\"{model_name}_assessment.json\")\n",
    "            with open(assessment_file, 'w') as f:\n",
    "                json.dump(final_assessment, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Assessment for {model_name} saved to {assessment_file}.\")\n",
    "        else:\n",
    "            print(f\"Failed to generate assessment for {model_name}.\")\n",
    "    else:\n",
    "        print(f\"No data available for model: {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109993,
     "status": "ok",
     "timestamp": 1736074875326,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "xihs6bQB_W8j",
    "outputId": "ae2e1863-8280-44a6-d3d7-2e975f774829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full assessment for model: llama\n",
      "Assessment for llama saved to /content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/full_model_assessments/llama_assessment.json.\n",
      "Generating full assessment for model: qwen\n",
      "Assessment for qwen saved to /content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/full_model_assessments/qwen_assessment.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def aggregate_evaluations(evaluation_folder):\n",
    "    \"\"\"Aggregate evaluations from multiple JSON files into model-specific data.\"\"\"\n",
    "    aggregated_scores = defaultdict(list)\n",
    "    aggregated_explanations = defaultdict(list)\n",
    "\n",
    "    # Iterate through all JSON evaluation files\n",
    "    evaluation_files = [f for f in os.listdir(evaluation_folder) if f.endswith('.json')]\n",
    "    for eval_file in evaluation_files:\n",
    "        file_path = os.path.join(evaluation_folder, eval_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            eval_data = json.load(f)\n",
    "\n",
    "            # Aggregate data for each model\n",
    "            for model, evaluation in eval_data.items():\n",
    "                if \"score\" in evaluation and evaluation[\"score\"] is not None:\n",
    "                    aggregated_scores[model].append(evaluation[\"score\"])\n",
    "                    aggregated_explanations[model].append(evaluation[\"explanation\"])\n",
    "\n",
    "    return aggregated_scores, aggregated_explanations\n",
    "\n",
    "\n",
    "def generate_full_assessment(aggregated_scores, aggregated_explanations, model_name, api_key):\n",
    "    \"\"\"Generate a full model assessment using the ChatGPT API.\"\"\"\n",
    "    explanations_text = \"\\n\\n\".join(aggregated_explanations[model_name])\n",
    "    average_score = sum(aggregated_scores[model_name]) / len(aggregated_scores[model_name])\n",
    "\n",
    "    # Create evaluation prompt\n",
    "    assessment_prompt = f\"\"\"\n",
    "    Você deve fornecer uma avaliação geral para o modelo '{model_name}' com base nos dados a seguir:\n",
    "\n",
    "    - Pontuação média: {average_score:.2f}\n",
    "    - Explicações agregadas:\n",
    "    {explanations_text}\n",
    "\n",
    "    Avalie os seguintes aspectos:\n",
    "    1. Os pontos mais fortes do modelo.\n",
    "    2. Os pontos mais fracos do modelo.\n",
    "    3. Recomendações para melhoria.\n",
    "    4. Um resumo geral da performance.\n",
    "\n",
    "    Forneça a resposta no seguinte formato:\n",
    "    {{\n",
    "        \"strengths\": [\"<forte1>\", \"<forte2>\", ...],\n",
    "        \"weaknesses\": [\"<fraqueza1>\", \"<fraqueza2>\", ...],\n",
    "        \"recommendations\": [\"<recomendação1>\", \"<recomendação2>\", ...],\n",
    "        \"summary\": \"<resumo>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call ChatGPT API\n",
    "    try:\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "        api_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Você é um avaliador expert em modelos de linguagem.\"},\n",
    "                {\"role\": \"user\", \"content\": assessment_prompt}\n",
    "            ]\n",
    "        )\n",
    "        assessment_result = api_response['choices'][0]['message']['content'].strip()\n",
    "        return json.loads(assessment_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating assessment for {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main Workflow\n",
    "evaluation_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/contextualized/\"\n",
    "output_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/full_model_assessments/\"\n",
    "api_key = \"example_key\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Aggregate evaluations from all JSON files\n",
    "aggregated_scores, aggregated_explanations = aggregate_evaluations(evaluation_folder)\n",
    "\n",
    "# Generate assessments for each model\n",
    "for model_name in aggregated_scores.keys():\n",
    "    print(f\"Generating full assessment for model: {model_name}\")\n",
    "\n",
    "    # Generate the full assessment\n",
    "    if aggregated_scores[model_name] and aggregated_explanations[model_name]:\n",
    "        final_assessment = generate_full_assessment(aggregated_scores, aggregated_explanations, model_name, api_key)\n",
    "\n",
    "        # Save the assessment to a separate file\n",
    "        if final_assessment:\n",
    "            assessment_file = os.path.join(output_folder, f\"{model_name}_assessment.json\")\n",
    "            with open(assessment_file, 'w') as f:\n",
    "                json.dump(final_assessment, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Assessment for {model_name} saved to {assessment_file}.\")\n",
    "        else:\n",
    "            print(f\"Failed to generate assessment for {model_name}.\")\n",
    "    else:\n",
    "        print(f\"No data available for model: {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1736075787892,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "_4h4D2aZLZ1D",
    "outputId": "5fadb149-56cc-490b-cac6-16f003d3d976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strengths': [\"O modelo consegue, em alguns casos, utilizar o formato de listagem com '-' conforme solicitado.\",\n",
       "  'A capacidade de sintetizar informações em tópicos, embora inconsistente, é evidente em algumas das respostas adequadas.',\n",
       "  'Quando adere ao formato solicitado, o modelo apresenta respostas concisas e diretas.'],\n",
       " 'weaknesses': ['Inconsistência na precisão e relevância dos tópicos listados em relação ao texto original.',\n",
       "  'Frequentemente, o modelo excede o limite de cinco palavras por tópico e não segue a estrutura de listagem com hífens de forma consistente.',\n",
       "  'Os tópicos listados muitas vezes são vagos, genéricos ou não capturam os elementos essenciais do texto original, levando a uma compreensão superficial ou imprecisa do conteúdo discutido.',\n",
       "  'Falha em adequar-se estritamente às instruções detalhadas, especialmente em manter cada tópico dentro do limite de palavras e em formatar corretamente conforme o pedido.'],\n",
       " 'recommendations': ['Melhorar o mecanismo de interpretação do texto para extrair tópicos mais precisos e relevantes, evitando generalizações.',\n",
       "  'Desenvolver uma maior aderência aos requisitos de formatação, garantindo que todos os tópicos sigam o formato de lista com hífens e se limitem a cinco palavras.',\n",
       "  'Intensificar o treinamento do modelo para entender e seguir estritamente as diretrizes dadas nos prompts, particularmente nos aspectos de limitação de palavras e estruturação de tópicos.'],\n",
       " 'summary': 'A performance do modelo revela uma capacidade variável de entender e seguir instruções, resultando em uma performance inconsistente. Enquanto as respostas corretas demonstram uma adequada compressão e concisão, frequentemente o modelo falha em manter a relevância e precisão exigida pelos prompts. Isso aponta para a necessidade de melhorias tanto na análise textual para extrair tópicos mais pertinentes quanto na estrita aderência ao formato exigido. O ajuste fino dessas áreas poderia significativamente elevar a qualidade e precisão das respostas geradas.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/full_model_assessments/qwen_assessment.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "  assessment = json.load(f)\n",
    "assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_V81Due7hE-"
   },
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84077,
     "status": "ok",
     "timestamp": 1735930880004,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "gWseBJAqdVws",
    "outputId": "4e578039-f16f-498a-b8fd-9f650e098ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Path to the evaluation results folder\n",
    "output_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Outputs/results/judge_chatgpt/\"\n",
    "\n",
    "# Function to read and aggregate explanations\n",
    "def aggregate_explanations(folder):\n",
    "    llama_explanations = []\n",
    "    qwen_explanations = []\n",
    "\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('_evaluation.json')]\n",
    "\n",
    "    for file in files:\n",
    "        with open(os.path.join(folder, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if \"llama\" in data and \"explanation\" in data[\"llama\"]:\n",
    "                llama_explanations.append(data[\"llama\"][\"explanation\"])\n",
    "            if \"qwen\" in data and \"explanation\" in data[\"qwen\"]:\n",
    "                qwen_explanations.append(data[\"qwen\"][\"explanation\"])\n",
    "\n",
    "    return llama_explanations, qwen_explanations\n",
    "\n",
    "# Function to summarize explanations\n",
    "def summarize_explanations(explanations, model_name, api_key):\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in evaluating machine learning models. Summarize the following evaluations for the {model_name} model.\n",
    "\n",
    "    Evaluations:\n",
    "    {explanations}\n",
    "\n",
    "    Provide a detailed analysis of the model's characteristics, positive points, and negative points. Be concise but thorough.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI model evaluation expert.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing explanations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Aggregate explanations\n",
    "llama_explanations, qwen_explanations = aggregate_explanations(output_folder)\n",
    "\n",
    "# Summarize using the API\n",
    "api_key = \"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\"\n",
    "llama_summary = summarize_explanations(\" \".join(llama_explanations), \"llama\", api_key)\n",
    "qwen_summary = summarize_explanations(\" \".join(qwen_explanations), \"qwen\", api_key)\n",
    "\n",
    "# Save summaries to file\n",
    "with open(os.path.join(output_folder, \"summary_llama.txt\"), 'w') as f:\n",
    "    f.write(llama_summary)\n",
    "\n",
    "with open(os.path.join(output_folder, \"summary_qwen.txt\"), 'w') as f:\n",
    "    f.write(qwen_summary)\n",
    "\n",
    "print(\"Summaries generated and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1735930880005,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "Kif178kidVkg",
    "outputId": "b37c55f4-e914-4bc7-81d6-eb2e30c62342"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"### Analysis of the Llama Model's Evaluation Responses\\n\\n#### Positive Points:\\n1. **Clarity in Enumerating Topics:** The model is often clear in listing relevant topics discussed during the teleconferences or events, indicating an ability to identify key themes from the text.\\n2. **Structural Organization:** When it works well, the model's responses are organized in a format that separates topics neatly, which could help in scanning and locating information quickly.\\n3. **Comprehension of General Content:** The model can grasp general information about the content, such as recognizing the nature of the document being a financial teleconference or an official business presentation.\\n\\n#### Negative Points:\\n1. **Lack of Depth and Detail:** The model's responses are often superficial and fail to delve into the specifics and intricacies of financial discussions, missing out on critical details that were part of the original transcripts.\\n2. **Failure to Adhere to Format Specifications:** The model frequently does not follow the detailed analytical or summary format stipulated by the task. Instead of providing in-depth analyses or comprehensive summaries, it tends to list topics without thorough development or explanation.\\n3. **Poor Contextualization and Analysis:** The model lacks in providing nuanced interpretation or critical analysis of the discussion points. There is a significant gap in connecting the dots between different parts of the discussion, and it does not evaluate the implications or outcomes of the discussed topics adequately.\\n4. **Redundancy and Generic Responses:** Responses from the model sometimes appear generic and could apply to various scenarios unrelated to the specific text provided. There's also an issue of repeating generic phrases that do not enhance the understanding of the text.\\n5. **Incompleteness and Ambiguity:** Some responses end abruptly or include vague terms like 'Positive' without sufficient explanation, making it hard for users to derive meaningful conclusions or insights.\\n\\nIn summary, while the model displays a basic ability to identify and list out discussion points from provided texts, it struggles significantly with depth, detailed analysis, and adherence to specific formats required by the tasks. Its responses lack the required specificity and depth to make them useful for an insightful understanding of financial discussions or corporate strategies as outlined in the transcripts. This evaluation underscores the need for improvements in the model's analytical capabilities to enhance its utility in accurately processing and summarizing complex financial content.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q10chwIpgL4q"
   },
   "outputs": [],
   "source": [
    "# Paths to the summary files\n",
    "llama_summary_path = os.path.join(output_folder, \"summary_llama.txt\")\n",
    "qwen_summary_path = os.path.join(output_folder, \"summary_qwen.txt\")\n",
    "\n",
    "# Read and print summaries\n",
    "with open(llama_summary_path, 'r') as f:\n",
    "    llama_summary = f.read()\n",
    "    print(\"Llama Summary:\\n\")\n",
    "    print(llama_summary)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "with open(qwen_summary_path, 'r') as f:\n",
    "    qwen_summary = f.read()\n",
    "    print(\"Qwen Summary:\\n\")\n",
    "    print(qwen_summary)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1735940053922,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "djspry9VFqir",
    "outputId": "4b5c0da3-8ccf-446d-afa2-ae399deb2654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analysis of the Llama Model's Evaluation Responses\n",
      "\n",
      "#### Positive Points:\n",
      "1. **Clarity in Enumerating Topics:** The model is often clear in listing relevant topics discussed during the teleconferences or events, indicating an ability to identify key themes from the text.\n",
      "2. **Structural Organization:** When it works well, the model's responses are organized in a format that separates topics neatly, which could help in scanning and locating information quickly.\n",
      "3. **Comprehension of General Content:** The model can grasp general information about the content, such as recognizing the nature of the document being a financial teleconference or an official business presentation.\n",
      "\n",
      "#### Negative Points:\n",
      "1. **Lack of Depth and Detail:** The model's responses are often superficial and fail to delve into the specifics and intricacies of financial discussions, missing out on critical details that were part of the original transcripts.\n",
      "2. **Failure to Adhere to Format Specifications:** The model frequently does not follow the detailed analytical or summary format stipulated by the task. Instead of providing in-depth analyses or comprehensive summaries, it tends to list topics without thorough development or explanation.\n",
      "3. **Poor Contextualization and Analysis:** The model lacks in providing nuanced interpretation or critical analysis of the discussion points. There is a significant gap in connecting the dots between different parts of the discussion, and it does not evaluate the implications or outcomes of the discussed topics adequately.\n",
      "4. **Redundancy and Generic Responses:** Responses from the model sometimes appear generic and could apply to various scenarios unrelated to the specific text provided. There's also an issue of repeating generic phrases that do not enhance the understanding of the text.\n",
      "5. **Incompleteness and Ambiguity:** Some responses end abruptly or include vague terms like 'Positive' without sufficient explanation, making it hard for users to derive meaningful conclusions or insights.\n",
      "\n",
      "In summary, while the model displays a basic ability to identify and list out discussion points from provided texts, it struggles significantly with depth, detailed analysis, and adherence to specific formats required by the tasks. Its responses lack the required specificity and depth to make them useful for an insightful understanding of financial discussions or corporate strategies as outlined in the transcripts. This evaluation underscores the need for improvements in the model's analytical capabilities to enhance its utility in accurately processing and summarizing complex financial content.\n"
     ]
    }
   ],
   "source": [
    "print(llama_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1735940135063,
     "user": {
      "displayName": "Arthur Barros",
      "userId": "09501105043826756631"
     },
     "user_tz": 180
    },
    "id": "X-KJ58guF9cT",
    "outputId": "d64a9e2b-f833-46d6-c6c0-9aebf2c21c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Análise Detalhada da Resposta do Modelo:**\n",
      "\n",
      "**Características Gerais:**\n",
      "- A resposta do modelo tenta abordar uma série de tópicos genericamente relacionados a uma teleconferência de resultados financeiros.\n",
      "- A estrutura da resposta é fragmentada, apresentando listagem de tópicos sem desenvolvimento substancial ou contextualização.\n",
      "- Falta de especificidade e profundidade analítica nos tópicos mencionados.\n",
      "\n",
      "**Pontos Positivos:**\n",
      "- O modelo consegue identificar e listar alguns elementos comuns discutidos em teleconferências de resultados como estratégias de negócios, discussões financeiras e Q&A (perguntas e respostas).\n",
      "- Mantém um nível básico de clareza ao enunciar categorias ou temas genéricos que poderiam ser discutidos durante uma conferência.\n",
      "\n",
      "**Pontos Negativos:**\n",
      "- **Falta de Profundidade:** A resposta não entra em detalhes sobre as discussões financeiras específicas, estratégias, dados econômicos ou respostas específicas às perguntas feitas durante a teleconferência. Não há análise crítica ou insights sobre as implicações dos pontos discutidos.\n",
      "- **Baixa Aderência ao Formato Solicitado:** Não segue o formato detalhado que tal análise exigiria. A resposta é mais uma enumeração de tópicos possíveis do que um resumo ou análise crítica direcionada da teleconferência.\n",
      "- **Clareza e Contextualização Insuficientes:** Falta contextualização adequada dos tópicos listados, tornando difícil para o leitor entender a relevância ou o impacto de cada tópico dentro da discussão maior da teleconferência. \n",
      "- **Resposta Genérica:** Os tópicos listados são tratados de maneira muito genérica, sem referências específicas ou citações do diálogo real na teleconferência, o que poderia validar a precisão e relevância dos pontos mencionados.\n",
      "- **Desconexão com o Texto Original:** A resposta não demonstra uma ligação clara ou direta com o texto original fornecido, parecendo mais uma resposta pré-formatada que não leva em conta as informações específicas da fonte.\n",
      "\n",
      "**Conclusão:**\n",
      "A resposta do modelo foi inadequada em atender de forma eficaz à tarefa de sumarizar e analisar detalhadamente uma teleconferência de resultados financeiros, mostrando uma falta significativa de profundidade, precisão e contextualização necessárias para uma análise útil e informativa. A capacidade de identificar corretamente e desenvolver pontos de discussão críticos, junto com uma análise precisa e bem fundamentada, é essencial para melhorar a eficácia da resposta em situações similares futuras.\n"
     ]
    }
   ],
   "source": [
    "print(qwen_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSJJhn5kFnm9"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS8arefhm6+fiDu2z+FIGc",
   "collapsed_sections": [
    "G_V81Due7hE-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
