{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAJMlSyBX3Hj"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfBBott7X22v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsRKO1yxXx2R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "!pip install pdfplumber\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjPMC9Z8X8QU"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Get Portuguese stop words\n",
        "portuguese_stop_words = set(stopwords.words('portuguese'))\n",
        "english_stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz5Q8_IqX-l-"
      },
      "outputs": [],
      "source": [
        "# As funções abaixo foram adaptadas de: https://github.com/jsvine/pdfplumber/issues/356#issuecomment-1471361607\n",
        "\n",
        "# Retorna se um objeto não está contido em outro\n",
        "# Por exemplo: se um texto está contido em uma tabela ou figura\n",
        "def not_within_bboxes(obj,bboxes):\n",
        "\n",
        "    def obj_in_bbox(_bbox):\n",
        "        v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
        "        h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
        "        x0, top, x1, bottom = _bbox\n",
        "        return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
        "\n",
        "    return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n",
        "\n",
        "def curves_to_edges(cs):\n",
        "    edges = []\n",
        "    for c in cs:\n",
        "        edges += pdfplumber.utils.rect_to_edges(c)\n",
        "    return edges\n",
        "\n",
        "# Extrai o texto de um arquivo PDF que não está dentro de tabelas ou figuras\n",
        "def raw_text_extract(pdf_file, include_tables=False, include_images=False, show_page_number=False):\n",
        "    page_data = []\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            #page = pdf.pages[0]\n",
        "\n",
        "            if show_page_number:\n",
        "                print(f\"Pagina: {page.page_number}\")\n",
        "\n",
        "            #print(page.extract_text())\n",
        "            bboxes = []\n",
        "            # identificando as tabelas\n",
        "            if not include_tables:\n",
        "                bboxes = [\n",
        "                    table.bbox\n",
        "                    for table in page.find_tables(\n",
        "                    table_settings={\n",
        "                        \"vertical_strategy\": \"lines\",\n",
        "                        \"horizontal_strategy\": \"lines\",\n",
        "                        \"explicit_vertical_lines\": curves_to_edges(page.curves) + page.edges,\n",
        "                        \"explicit_horizontal_lines\": curves_to_edges(page.curves) + page.edges,\n",
        "                    }\n",
        "                    )\n",
        "                ]\n",
        "            #print(bboxes)\n",
        "\n",
        "            # identificando as imagens\n",
        "            if not include_images:\n",
        "                for image in page.images:\n",
        "                    image_bbox = (image['x0'], image['top'], image['x1'], image['bottom'])\n",
        "                    bboxes.append(image_bbox)\n",
        "                    #print(\"img: \",image_bbox)\n",
        "\n",
        "            # Filtrando os textos que estão fora das caixas das tabelas e das figuras\n",
        "            page = page.filter(lambda obj: not_within_bboxes(obj, bboxes))\n",
        "\n",
        "            # a arquivo bbdc-2015-3T-Transcrição da Teleconferência 3T15.pdf gera caracteres duplicados. Esse método resolve o problema.\n",
        "            # ref: https://github.com/jsvine/pdfplumber/issues/71\n",
        "            text = page.dedupe_chars().extract_text()\n",
        "\n",
        "\n",
        "            ##### removendo cabeçalho: bbdc, bbas, prbc -> \"Transc... ano\": Pode ter ou não \"Transcrição da\"; Pode ter 3 ou 4 linhas\n",
        "\n",
        "            # Transcrição da Teleconferência\n",
        "            # Resultados do 4T09\n",
        "            # Banco do Brasil (BBAS3 BZ) <- esta linha pode nao aparecer no BBDC\n",
        "            # 26 de fevereiro de 2010\n",
        "\n",
        "            # prbc-2013-2T13\n",
        "            # Teleconferência do Paraná Banco\n",
        "            # Resultados do 2° trimestre de 2013\n",
        "            # 14 de junho de 2013 – 11h00 (horário de Brasília)\n",
        "            text = re.sub(r\"(Transcrição da )?Teleconferência(.*)?((?:\\n|\\r\\n?)(.*))?((?:\\n|\\r\\n?)(.*))?(?:\\n|\\r\\n?)(.*)[0-9]{4}( – [0-9]{2}h(.*))?\", '', text).strip()\n",
        "\n",
        "            ##### Removendo cabeçalhos do ITUB\n",
        "            # Itaú Unibanco\n",
        "            # Resultados do Terceiro trimestre de 2018\n",
        "            # 30 de outubro de 2018\n",
        "            text = re.sub(r\"Itaú Unibanco(.*)?((?:\\n|\\r\\n?)(.*))?((?:\\n|\\r\\n?)(.*))?(?:\\n|\\r\\n?)(.*)[0-9]{4}\", '', text).strip()\n",
        "\n",
        "            ##### Removendo cabeçalhos do BBAS\n",
        "            # - Palavra \"#Pública\" (3T19 a 3T22) e #interna (1T21 a 1T22)\n",
        "            text = re.sub(f\"#Pública\", '', text).strip()\n",
        "\n",
        "            # - Palavra \"#interna\" (1T21 a 1T22)\n",
        "            text = re.sub(f\"#interna\", '', text).strip()\n",
        "\n",
        "\n",
        "            # bbas-2006-4T06-Transcrição\n",
        "            # Local Conference Call\n",
        "            # Banco do Brasil Nac. – (29314)\n",
        "            # Resultados do Exercício de 2006\n",
        "            # 28 de Fevereiro de 2007 – 11:00h - horário local\n",
        "            text = re.sub(r\"Local Conference Call(?:\\n|\\r\\n?)(.*)?((?:\\n|\\r\\n?)(.*))?(?:\\n|\\r\\n?)(.*)\", '', text).strip()\n",
        "\n",
        "            # bbas-2009-4T09-Transcrição.pdf\n",
        "            # O padrão é este. As linhas iniciais são removidas na regra anterior, mas fica a linha da data, que é removida aqui\n",
        "            # Transcrição da Teleconferência\n",
        "            # Resultados do 4T09\n",
        "            # Banco do Brasil (BBAS3 BZ)\n",
        "            # 26 de fevereiro de 2010\n",
        "            text = re.sub(r\"^[0-9]{2}(.*)[0-9]{4}\", '', text).strip()\n",
        "\n",
        "            # Relação com Investidores\n",
        "            # Transcrição 1T21\n",
        "            # BANCO DO BRASIL\n",
        "            # TELECONFERÊNCIA\n",
        "            # DE RESULTADOS\n",
        "            # 1T2021\n",
        "            text = re.sub(r\"Relação com Investidores(?:\\n|\\r\\n?)(.*)?((?:\\n|\\r\\n?)(.*))?((?:\\n|\\r\\n?)(.*))?(?:\\n|\\r\\n?)(.*)?(?:\\n|\\r\\n?)(.*)\", '', text).strip()\n",
        "\n",
        "            # bbas-2020-3T20-Transcrição Teleconferência 3T20.pdf\n",
        "            # BANCO DO BRASIL\n",
        "            # TELECONFERÊNCIA\n",
        "            # DE RESULTADOS\n",
        "            # 3T2020\n",
        "            # 06/11/2020\n",
        "            text = re.sub(r\"BANCO DO BRASIL(?:\\n|\\r\\n?)(.*)?((?:\\n|\\r\\n?)(.*))?((?:\\n|\\r\\n?)(.*))?(?:\\n|\\r\\n?)(.*)\", '', text).strip()\n",
        "\n",
        "\n",
        "            # removendo cabeçalho: abcb -> \"Banco ABC Brasil | Relações com Investidores Transcrição da\"\n",
        "            text = re.sub(r\"Banco ABC Brasil \\| (.*)((?:\\n|\\r\\n?))\", '', text).strip()\n",
        "\n",
        "\n",
        "            # removendo o número da página que veio junto do texto\n",
        "            text = re.sub(f\"\\\\n{page.page_number}$\", '', text)\n",
        "\n",
        "            text = re.sub(f'- {page.page_number} -', '', text)\n",
        "\n",
        "\n",
        "            # removendo o número da página que veio junto do texto: bbdc -> \"(cid:1) <pagina>\"\n",
        "            text = re.sub(f\"\\(cid:1\\) {page.page_number}$\", '', text).strip()\n",
        "\n",
        "            # removendo o número da página que veio junto do texto: prbc -> \"Página <pagina>\"\n",
        "            text = re.sub(f\"Página {page.page_number}$\", '', text).strip()\n",
        "\n",
        "            # removendo o número da página que veio junto do texto: itub -> \"1/12\"\n",
        "            text = re.sub(f'{page.page_number}/{len(pdf.pages)}$', '', text).strip()\n",
        "\n",
        "            # removendo o número da página que veio junto do texto: itub -> \"Teleconferência 4T21 2\"\n",
        "            text = re.sub(r'Teleconferência \\dT\\d{2} \\d+$', '', text).strip()\n",
        "\n",
        "            # removendo o número da página que veio junto do texto:\n",
        "            # No abcp-2009-3T09, todas as páginas estão com número \"9\" e\n",
        "            # no abcp-2009-3T09, todas as páginas estão com número 7\n",
        "            head, tail = os.path.split(pdf_file)\n",
        "            if (tail.startswith('abcb-2009')):\n",
        "                if (tail.startswith('abcb-2009-3T09')):\n",
        "                    text = re.sub(r'((?:\\n|\\r\\n?))9$', '', text).strip()\n",
        "                elif (tail.startswith('abcb-2009-4T09')):\n",
        "                    text = re.sub(r'((?:\\n|\\r\\n?))7$', '', text).strip()\n",
        "\n",
        "            page_data.append(text)\n",
        "\n",
        "    return page_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf5nF_MCYAnf"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "\n",
        "    text = text.replace(\"\\n\", ' ')\n",
        "    text = text.replace(\"”\", '')\n",
        "    text = text.replace(\"“\", '')\n",
        "    text = text.replace(\"\\\"\", '')\n",
        "    text = text.replace(\"\", '')\n",
        "\n",
        "    #lista com bolinha\n",
        "    text = re.sub(r'(\\s)?(;\\s)?(•)', \"; \", text.strip())\n",
        "    text = re.sub(r'(: ;)', \": \", text.strip())\n",
        "    text = re.sub(r'(\\.;)', \". \", text.strip())\n",
        "\n",
        "    # nu-2021-4T21-Script 4T21.pdf\n",
        "    text = re.sub(r'(; -)', \". \", text.strip())\n",
        "    text = re.sub(r'(\\. -)', \". \", text.strip())\n",
        "    text = re.sub(r'^(-)', \"\", text.strip())\n",
        "    text = re.sub(r'(: \\d.)', ': .', text.strip())\n",
        "    text = re.sub(r'(; e (\\d+\\.)?)', '.', text.strip())\n",
        "    text = re.sub(r'(: ●)', '.', text.strip())\n",
        "    text = re.sub(r'(; ●)', '.', text.strip())\n",
        "    text = re.sub(r'(●)', '', text.strip())\n",
        "\n",
        "    text = re.sub(\"_______________________________________________________________\", \"\", text)\n",
        "\n",
        "    text = re.sub(r\"Sra\\.\", \"Senhora \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"Sr\\.\", \"Senhor \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"Srs\\.\", \"Senhores \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'b\\.p\\.\\s([A-Z])', 'bp. \\\\1', text).strip()\n",
        "    text = re.sub('b.p.', 'bp', text).strip()\n",
        "    text = re.sub('p.p.', 'pp', text).strip()\n",
        "    text = re.sub('help!', 'help', text).strip() # bmgb -> tirar a exclamação para evitar quebra de sentenças\n",
        "\n",
        "    text = re.sub('\\s+', ' ', text).strip() # deixar por ultimo, pois as substituicoes anteriores podem inserir multiplos espaços\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU7Ld5t-YAgs"
      },
      "outputs": [],
      "source": [
        "def get_sentences(text):\n",
        "    sentences = sent_tokenize(text, language='portuguese')\n",
        "    sentences = [s for s in sentences if len(s.strip()) > 2]\n",
        "    return sentences\n",
        "\n",
        "def get_tokens(text):\n",
        "    tokens = word_tokenize(text, language='portuguese')\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def get_words(text):\n",
        "    tokens = get_tokens(text)\n",
        "\n",
        "    words = [w for w in tokens if w not in string.punctuation]\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsltA65rYAYh"
      },
      "outputs": [],
      "source": [
        "def generate_stats(text):\n",
        "    stats = {}\n",
        "    tokens = get_tokens(text)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    stats[\"tokens\"] = num_tokens\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyokOKTCYKU8"
      },
      "source": [
        "# Split Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlOihOojYD_1"
      },
      "outputs": [],
      "source": [
        "def split_text(text):\n",
        "  # Define patterns to identify the start of the Q&A section\n",
        "  qna_start_patterns = [\n",
        "    \"Estamos abertos para perguntas que vocês possam ter\",\n",
        "    'Era basicamente isso que a gente tinha para falar, então a gente pode ir agora para as perguntas',\n",
        "    \"Encerrando a apresentação, eu gostaria agora de abrir para Perguntas e Respostas\"\n",
        "  ]\n",
        "\n",
        "  # Join all patterns into a single regex pattern\n",
        "  pattern = r\"|\".join(qna_start_patterns)\n",
        "\n",
        "  match = re.search(pattern, text, re.IGNORECASE)\n",
        "  if match:\n",
        "      split_index = match.start()\n",
        "      presentation = text[:split_index].strip()\n",
        "      qna = text[split_index:].strip()\n",
        "      return presentation, qna\n",
        "  else:\n",
        "      return text, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PzFc409YLrA"
      },
      "outputs": [],
      "source": [
        "transcript_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Transcripts/'\n",
        "transcript_text_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/transcricoes_processadas/\"\n",
        "\n",
        "global_stats = pd.DataFrame()\n",
        "\n",
        "#Recupera os arquivos que estão na pasta\n",
        "files = os.listdir(transcript_folder)\n",
        "files = files[0:2]\n",
        "\n",
        "num_files = len(files)\n",
        "for i, file in enumerate(files):\n",
        "    path_transcription = transcript_folder + file\n",
        "\n",
        "    file_name_parts = file.split(\"-\")\n",
        "    ticker          = file_name_parts[0].strip()\n",
        "    trimestre       = file_name_parts[2].strip()\n",
        "\n",
        "    path_folder_ticker = os.path.join(transcript_text_folder, ticker)\n",
        "\n",
        "    print(path_folder_ticker, ticker, trimestre)\n",
        "    print('path_transcription', path_transcription)\n",
        "    if not os.path.exists(path_folder_ticker):\n",
        "        os.mkdir(path_folder_ticker)\n",
        "\n",
        "    print('is_file',os.path.isfile(path_transcription))\n",
        "    if os.path.isfile(path_transcription):\n",
        "      print(f\"Processando arquivo {i+1} de {num_files}\")\n",
        "\n",
        "      print(\"*** Extraindo texto do PDF: \"+ file)\n",
        "      path_transcription = transcript_folder + file\n",
        "      if ticker in ['bbas', 'bbdc']:\n",
        "          page_data = raw_text_extract(path_transcription)\n",
        "      else:\n",
        "          page_data = raw_text_extract(path_transcription, include_tables=True, include_images=False)\n",
        "\n",
        "      #print(page_data)\n",
        "\n",
        "      print(\"*** Limpando o texto\")\n",
        "      text = \" \".join(page_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnnkKDTqYXT5"
      },
      "outputs": [],
      "source": [
        "#!pip install openai\n",
        "#!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n",
        "import openai\n",
        "api_key = \"api_key\"\n",
        "openai.api_key = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwt4btbFYYfy"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imnc2FK1YaLi"
      },
      "outputs": [],
      "source": [
        "# Define the function to evaluate the text with a prompt\n",
        "def evaluate_text_with_prompt(text, prompt, model=\"gpt-4-turbo\"):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Você deve concentrar os outputs para serem estritamente o que pedi, pois vou usá-los para códigos em pipeline\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOB-Phu4YcWJ"
      },
      "outputs": [],
      "source": [
        "transcript_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Transcripts/'\n",
        "transcript_text_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Generated Text 2/\"\n",
        "\n",
        "#Recupera os arquivos que estão na pasta\n",
        "files = os.listdir(transcript_text_folder)\n",
        "files = sorted(files)\n",
        "\n",
        "cutoff = {}\n",
        "\n",
        "for file_path in files:\n",
        "  path_transcription = transcript_text_folder + file_path\n",
        "\n",
        "  file_name_parts = file_path.split(\"-\")\n",
        "  ticker          = file_name_parts[0].strip()\n",
        "  ano             = file_name_parts[1].strip()\n",
        "  trimestre       = file_name_parts[2].strip()[0]\n",
        "\n",
        "  print(ticker,ano,trimestre)\n",
        "\n",
        "  with open(path_transcription, 'r') as file:\n",
        "    text_content = file.read()\n",
        "\n",
        "    # Example prompt\n",
        "    prompt = f\"\"\"\n",
        "      Você poderia indicar o trecho do texto que posso usar como cutoff para separar o texto da apresentação e o das perguntas e repostas (Q&A)?\n",
        "\n",
        "      Por favor não inclua nada do enunciado na resposta, apenas o trecho do texto até a primeira pontuação do Q&A.\n",
        "    \"\"\"\n",
        "\n",
        "    response = evaluate_text_with_prompt(text_content, prompt) #evaluate_text_with_prompt(before_text, prompt)\n",
        "\n",
        "    cutoff[f'{ticker}-{ano}-{trimestre}'] = response\n",
        "\n",
        "df_cutoff = pd.DataFrame.from_dict(cutoff, orient='index', columns=['cutoff'])\n",
        "df_cutoff.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOzSFaBYofe"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Temp'\n",
        "df_cutoff.to_csv(f'{file_path}/df_cutoff_raw.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wnm3O0QYxgS"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "df_cutoff['revision'] = 0\n",
        "for index, row in df_cutoff.iterrows():\n",
        "  i = i+1\n",
        "  print(i+1)\n",
        "\n",
        "      # Example prompt\n",
        "  prompt = f\"\"\"\n",
        "    Você poderia me indicar se o trecho indica que ainda haverá uma fala antes do início da sessão de perguntas e resposta?\n",
        "\n",
        "    Gostaria que a resposta viesse como uma opção binária: (Sim) ou (Não).\n",
        "  \"\"\"\n",
        "\n",
        "  print(text_content, prompt)\n",
        "\n",
        "  response = evaluate_text_with_prompt(row['cutoff'], prompt) #evaluate_text_with_prompt(before_text, prompt)\n",
        "  print(ticker, ano, trimestre, response)\n",
        "\n",
        "  df_cutoff.loc[index,'revision'] = response\n",
        "\n",
        "df_cutoff.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0bvxBJAY9fm"
      },
      "outputs": [],
      "source": [
        "#Search File name saved\n",
        "#file_path = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Temp'\n",
        "#df_cutoff.to_csv(f'{file_path}/df_cutoff_raw.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMuYb-c4Y6YZ"
      },
      "outputs": [],
      "source": [
        "transcript_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Transcripts/'\n",
        "transcript_text_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/Generated Text 2/\"\n",
        "\n",
        "#Recupera os arquivos que estão na pasta\n",
        "files = os.listdir(transcript_text_folder)\n",
        "files = sorted(files)\n",
        "\n",
        "revision_list = list(df_cutoff[df_cutoff['revision'] == 'Sim']['doc'])\n",
        "\n",
        "revised_cutoff = {}\n",
        "\n",
        "#banco do brasil: files[59:125]\n",
        "#for file_path in files:\n",
        "for file_path in files:\n",
        "  print('file_path',file_path)\n",
        "  path_transcription = transcript_text_folder + file_path\n",
        "\n",
        "  file_name_parts = file_path.split(\"-\")\n",
        "  print(file_name_parts)\n",
        "  ticker          = file_name_parts[0].strip()\n",
        "  ano             = file_name_parts[1].strip()\n",
        "  trimestre       = file_name_parts[2].strip()[0]\n",
        "\n",
        "  if f'{ticker}-{ano}-{trimestre}' in revision_list:\n",
        "    print(ticker,ano,trimestre, 'YES')\n",
        "\n",
        "    with open(path_transcription, 'r') as file:\n",
        "      text_content = file.read()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "      Você poderia indicar o trecho do texto que posso usar como cutoff para separar o texto da apresentação e o das perguntas e repostas (Q&A)?\n",
        "\n",
        "      Certifique-se de que não haja nenhuma seção depois do trecho e antes das perguntas e respostas. Mesmo que seja apenas um comentário ou última explicação.\n",
        "\n",
        "      Se o trecho anunciar que haverá alguma fala antes das perguntas e respostas ele não está correto.\n",
        "\n",
        "      Por favor não inclua nada do enunciado na resposta, apenas o trecho do texto até a primeira pontuação do Q&A.\n",
        "    \"\"\"\n",
        "\n",
        "    print(text_content, prompt)\n",
        "\n",
        "    response = evaluate_text_with_prompt(text_content, prompt) #evaluate_text_with_prompt(before_text, prompt)\n",
        "    print(ticker, ano, trimestre, response)\n",
        "\n",
        "    revised_cutoff[f'{ticker}-{ano}-{trimestre}'] = response\n",
        "\n",
        "df_revised_cutoff = pd.DataFrame.from_dict(revised_cutoff, orient='index', columns=['cutoff'])\n",
        "df_revised_cutoff.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn2YlUyuZTyX"
      },
      "outputs": [],
      "source": [
        "df_cutoff_merge = df_cutoff.merge(df_revised_cutoff, on='doc', how='left', suffixes=('_original', '_revised'))\n",
        "df_cutoff_merge['cutoff_revised'] = df_cutoff_merge.apply(lambda x: x['cutoff_revised'] if x['revision'] == 'Sim' else x['cutoff_original'],axis=1)\n",
        "df_cutoff_merge.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bwF4X3mZXSJ"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Temp'\n",
        "df_cutoff_merge.to_csv(f'{file_path}/df_cutoff_merge.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec0s6JwWZfN3"
      },
      "source": [
        "## Manual Adjustments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw-CL95gZi0e"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Temp/df_cutoff_merge - antigo.csv'\n",
        "df_cutoff_merge = pd.read_csv(file_path)\n",
        "df_cutoff_merge.drop(columns=df_cutoff_merge.columns[0], axis=1)\n",
        "df_cutoff_merge.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyAvq54PZ_38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# Example folder paths\n",
        "transcript_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Transcripts/'\n",
        "transcript_text_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/transcricoes_processadas_BERTopic/\"\n",
        "\n",
        "# Suppose these are your files\n",
        "files = os.listdir(transcript_folder)\n",
        "files = files#[:10]  # Just first 10 as an example\n",
        "\n",
        "def process_single_file(file_name, df_cutoff_merge):\n",
        "    \"\"\"\n",
        "    Worker function that processes a single file.\n",
        "    Returns a dictionary with doc name, 'presentation', 'qna' segments, and their lengths.\n",
        "    \"\"\"\n",
        "    path_transcription = os.path.join(transcript_folder, file_name)\n",
        "\n",
        "    # Split file name to extract relevant parts\n",
        "    file_name_parts = file_name.split(\"-\")\n",
        "    ticker    = file_name_parts[0].strip()\n",
        "    ano       = file_name_parts[1].strip()\n",
        "    trimestre = file_name_parts[2].strip()\n",
        "\n",
        "    # Decide on how to call raw_text_extract based on ticker\n",
        "    if ticker in ['bbas', 'bbdc']:\n",
        "        page_data = raw_text_extract(path_transcription)\n",
        "    else:\n",
        "        page_data = raw_text_extract(path_transcription, include_tables=True, include_images=False)\n",
        "\n",
        "    # JOIN the list of lines/pages into a single string\n",
        "    text = \" \".join(page_data)\n",
        "\n",
        "    # Build the doc_name for lookup in df_cutoff_merge\n",
        "    doc_name = f'{ticker}-{ano}-{trimestre[0]}'\n",
        "\n",
        "    # Retrieve the cutoff pattern for this doc\n",
        "    # IMPORTANT: Make sure doc_name actually matches something in df_cutoff_merge\n",
        "    # e.g. 'bbas-:2023-1'\n",
        "    cutoff_rows = df_cutoff_merge[df_cutoff_merge['doc'] == doc_name]\n",
        "    if len(cutoff_rows) == 0:\n",
        "        # If no matching row, skip or handle logic\n",
        "        # For now, treat entire text as presentation\n",
        "        return {\n",
        "            'doc': doc_name,\n",
        "            'presentation': text,\n",
        "            'qna': None,\n",
        "            'presentation_len': len(text),\n",
        "            'qna_len': 0\n",
        "        }\n",
        "\n",
        "    cutoff_str = cutoff_rows['joint_cutoff'].values[0]\n",
        "    print(doc_name, cutoff_str)\n",
        "\n",
        "    # Escape special regex chars to avoid unbalanced parentheses or similar\n",
        "    pattern = re.escape(cutoff_str)\n",
        "\n",
        "    # Search for the pattern in the text\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        split_index = match.start()\n",
        "        presentation = text[:split_index].strip()\n",
        "        qna = text[split_index:].strip()\n",
        "    else:\n",
        "        # If no match, entire text is 'presentation'\n",
        "        presentation = text\n",
        "        qna = None\n",
        "\n",
        "    # Create a row-like dictionary\n",
        "    new_row = {\n",
        "        'doc': doc_name,\n",
        "        'cutoff': cutoff_str,\n",
        "        'presentation': presentation,\n",
        "        'qna': qna,\n",
        "        'presentation_len': len(presentation),\n",
        "        'qna_len': len(qna) if qna else 0\n",
        "    }\n",
        "    return new_row\n",
        "\n",
        "# Prepare an empty DataFrame to gather results\n",
        "df_divided = pd.DataFrame(columns=['doc','presentation','qna','presentation_len','qna_len'])\n",
        "\n",
        "num_files = len(files)\n",
        "texts_as_strings = []\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    # Use functools.partial to pass df_cutoff_merge\n",
        "    process_func = partial(process_single_file, df_cutoff_merge=df_cutoff_merge)\n",
        "\n",
        "    # Executor map returns an iterator of dictionaries (one per file)\n",
        "    results = executor.map(process_func, files)\n",
        "\n",
        "    for i, row_dict in enumerate(results, start=1):\n",
        "        print(f\"Processed file {i} of {num_files}\")\n",
        "\n",
        "        # row_dict is the dictionary returned by process_single_file\n",
        "        # We can build texts_as_strings if needed\n",
        "        presentation_text = row_dict['presentation'] if row_dict['presentation'] else \"\"\n",
        "        qna_text = row_dict['qna'] if row_dict['qna'] else \"\"\n",
        "        combined_text = presentation_text + \" \" + qna_text\n",
        "        texts_as_strings.append(combined_text)\n",
        "\n",
        "        # Append row to df_divided\n",
        "        df_divided = pd.concat([df_divided, pd.DataFrame([row_dict])], ignore_index=True)\n",
        "\n",
        "print(\"All files processed.\")\n",
        "print(df_divided.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPq21dPRZ_xl"
      },
      "outputs": [],
      "source": [
        "df_divided['cutoff_revised'] = df_divided['cutoff']\n",
        "#abcb\n",
        "df_divided.iloc[226,6] = 'É basicamente isso, estamos abertos para perguntas que vocês possam ter.'\n",
        "#bbas\n",
        "df_divided.iloc[22,6]  = 'Operadora: Obrigada. Com licença, iniciaremos agora a sessão de perguntas e respostas.'\n",
        "df_divided.iloc[283,6] = 'OPERADORA – Senhoras e senhores, iniciaremos agora a sessão de perguntas e respostas. Para fazer uma pergunta, por favor, digitem *1.'\n",
        "df_divided.iloc[253,6] = 'OPERADORA – Senhoras e senhores, iniciaremos agora a Sessão de Perguntas e Respostas. Para fazer uma pergunta, por favor, digitem *1.'\n",
        "df_divided.iloc[347,6] = 'OPERADORA- Senhoras e senhores, iniciaremos agora a sessão de perguntas e respostas.'\n",
        "df_divided.iloc[143,6] = 'OPERADORA – Obrigada. Iniciaremos agora a sessão de perguntas e respostas'\n",
        "df_divided.iloc[215,6] = 'Agora me junto ao Cassiano e ao Firetti para a sessão de perguntas e respostas.'\n",
        "#bmgb\n",
        "df_divided.iloc[191,6] = 'gostaria de abrir para sessões de perguntas e respostas'\n",
        "df_divided.iloc[295,6] = 'Danilo, abro agora a sessão de perguntas.'\n",
        "df_divided.iloc[92,6]  = 'isso, encerro também a apresentação e abro para a sessão de perguntas.'\n",
        "df_divided.iloc[131,6]  = 'Com isso, encerramos a nossa apresentação e vamos passar para perguntas e respostas'\n",
        "df_divided.iloc[383,6]  = 'Com isso, encerramos aqui a nossa apresentação e vamos abrir agora para perguntas'\n",
        "df_divided.iloc[219,6]  = 'Dessa forma, eu encerro aqui a apresentação de resultados do Banco Bmg e'\n",
        "df_divided.iloc[9,6]  = 'Encerro aqui a apresentação. Iniciaremos a nossa sessão de perguntas e respostas'\n",
        "df_divided.iloc[296,6]  = 'E assim, eu encerro aqui a apresentação de resultados, para iniciarmos a sessão, agora'\n",
        "df_divided.iloc[352,6]  = 'Vamos passar para o Q&A agora.'\n",
        "df_divided.iloc[89,6]  = 'E assim, eu encerro a apresentação e gostaria de abrir para sessões de perguntas'\n",
        "#bpan\n",
        "df_divided.iloc[274,6]  = 'E aí, com isso, a gente encerra os slides e pode começar o nosso Q&A com os analistas e investidores'\n",
        "#banrisul\n",
        "df_divided.iloc[90,6]  = 'Então, acho que era isso que eu queria comentar, e estamos à disposição para perguntas dos senhores'\n",
        "df_divided.iloc[6,6]  = 'Eram essas as observações que eu tinha a fazer, e agora aguardamos os questionamentos para ver as questões que nós podemos clarear. Obrigado'\n",
        "df_divided.iloc[113,6]  = 'Vou encerrar por enquanto, e o grupo aqui ficará à disposição para eventuais perguntas de quem está nos prestigiando aqui hoje. Muito obrigado.'\n",
        "df_divided.iloc[236,6]  = 'eu só queria fazer isso com essa consideração final e já podemos passar, então, para as perguntas e respostas.'\n",
        "df_divided.iloc[66,6]  = 'Com isso, a apresentação em si se encerra e devolvemos a palavra aos senhores e às senhoras para as perguntas que desejarem. Ficamos à vontade.'\n",
        "df_divided.iloc[19,6]  = 'Com isso, eu encerro a discussão dos principais números, e nós nos colocamos à disposição dos analistas para os questionamentos que houver. Obrigado.'\n",
        "df_divided.iloc[204,6]  = 'Com isso, vamos passar para a parte dos questionamentos do mercado, para'\n",
        "df_divided.iloc[199,6]  = 'E agradeço a todos pela participação nessa áudioconferência. Obrigado'\n",
        "df_divided.iloc[323,6]  = 'Com isso aqui eu encerro a apresentação e devolvo para a continuidade da nossa reunião.'\n",
        "#itub\n",
        "df_divided.iloc[188,6]  = 'Com isso encerramos a apresentação e estamos aqui abertos todos nós para as perguntas que vocês quiserem fazer.'\n",
        "df_divided.iloc[85,6]  = 'Com isso eu encerro a apresentação. Nós estamos aqui todos abertos às perguntas que vocês queiram fazer.'\n",
        "df_divided.iloc[310,6]  = 'Essas são as telas que a gente estaria apresentando, agora estamos abertos às perguntas de vocês.'\n",
        "df_divided.iloc[162,6]  = 'Muito bem, tendo dito tudo isto, eu estou abrindo para perguntas e obrigado aí pela atenção de vocês.'\n",
        "df_divided.iloc[355,6]  = 'Com isso eu encerro aqui a apresentação e estaremos a partir de agora, o Marcelo Kopel e eu, à disposição para responder eventuais perguntas. Obrigado.'\n",
        "df_divided.iloc[291,6]  = 'Bem, com estas projeções, eu concluo a apresentação e nós podemos passar para as perguntas e respostas.'\n",
        "df_divided.iloc[4,6]  = 'Com isso, nós concluímos esta apresentação e estamos abertos agora a qualquer questão que vocês possam ter. Muito obrigado.'\n",
        "df_divided.iloc[194,6]  = 'Com isto, eu concluo a apresentação, estou aberto para qualquer dúvida que vocês possam ter. Muito obrigado.'\n",
        "df_divided.iloc[250,6]  = 'Com isso, nós concluímos esta apresentação e estamos abertos agora a qualquer questão que vocês possam ter. Muito obrigado.'\n",
        "df_divided.iloc[232,6]  = 'E, com isso, eu encerro aqui a apresentação e abro para perguntas.'\n",
        "df_divided.iloc[322,6]  = 'chamar o Renato de volta para me ajudar com a sessão de perguntas e respostas.'\n",
        "df_divided.iloc[377,6]  = 'Agora vou para o nosso Perguntas e Respostas, vou me juntar ao Renato. Nos vemos'\n",
        "#prbc\n",
        "df_divided.iloc[234,6]  = 'Não existe'\n",
        "df_divided.iloc[177,6]  = 'Operador: Nossa primeira pergunta vem do senhor Francisco do banco Safra.'\n",
        "#santander\n",
        "df_divided.iloc[251,6]  = 'Então, muito obrigado pela sua atenção. Quando vocês quiserem, podemos'\n",
        "df_divided.iloc[38,6]  = 'Bom dia. Será que vocês poderiam nos dar um pouco mais de detalhe da natureza'\n",
        "df_divided.iloc[329,6]  = 'Com isso, eu concluo, e creio que agora teremos nossa sessão de perguntas e respostas. Muito obrigado.'\n",
        "df_divided.iloc[218,6]  = 'Obrigado por sua atenção e agora estamos disponíveis para responder às suas perguntas.'\n",
        "df_divided.iloc[245,6]  = 'Eu gostaria de agradecer a todos pela atenção. Agora podemos iniciar a sessão de perguntas e respostas.'\n",
        "df_divided.iloc[17,6]  = 'Gostaria de agradecer a todos pela atenção e ficamos à disposição responder as suas perguntas'\n",
        "df_divided.iloc[195,6]  = 'Então, era isso, em poucas palavras, que eu queria compartilhar com vocês, e acho que agora podemos abrir a sessão para perguntas e respostas.'\n",
        "df_divided.iloc[21,6]  = 'Com isso, faço uma pausa e teremos, acredito, meia hora para perguntas e respostas'\n",
        "#abcb\n",
        "df_divided.iloc[102,6]  = 'Esses eram os principais fatos que a gente gostaria de apresentar, e nos colocamos à disposição para qualquer dúvida ou qualquer pergunta'\n",
        "df_divided.iloc[197,6]  = 'a gente tinha para apresentar, a gente abre para qualquer dúvida, pra qualquer pergunta, ahn, que, que seja necessária. Obrigado.'\n",
        "df_divided.iloc[87,6]  = 'Isso é basicamente o que a gente apostou, então a partir de agora a gente abre para responder às perguntas que vocês'\n",
        "df_divided.iloc[109,6]  = 'Oi pessoal, é o Thiago Baptista. Eu tenho 2 perguntas. A primeira em relação as margens'\n",
        "df_divided.iloc[357,6]  = 'Estes são os dados que gostaríamos de apresentar. Nos colocamos agora à disposição para responder'\n",
        "df_divided.iloc[171,6]  = 'Para participar é só levantar a mão clicando no ícone na parte inferior da tela'\n",
        "df_divided.iloc[64,6]  = 'transmissão. Agora, como mencionado, abriremos a seção de perguntas e respostas.'\n",
        "#bbas\n",
        "df_divided.iloc[137,6]  = 'Essas eram as considerações sobre detalhamento dos resultados. Gostaríamos agora de abrir a teleconferência para a sessão de perguntas e respostas'\n",
        "df_divided.iloc[220,6]  = 'Agora podemos abrir a sessão de perguntas e respostas, e ficamos à disposição'\n",
        "df_divided.iloc[269,6]  = 'Essas eram praticamente as informações que nós gostaríamos de compartilhar com vocês. Podemos agora abri para a sessão de perguntas e respostas. Muito obrigada.'\n",
        "df_divided.iloc[373,6]  = 'Nossa primeira pergunta vem do senhor Eduardo Rosman, do Banco'\n",
        "df_divided.iloc[255,6]  = 'Agradeço agora a presença de todos e podemos seguir para a sessão de perguntas e respostas'\n",
        "df_divided.iloc[261,6]  = 'E agora agradeço a participação de todos e podemos iniciar a sessão de perguntas e respostas'\n",
        "df_divided.iloc[115,6]  = 'Assim eu finalizo aqui a apresentação dos nossos números e gostaria de abrir, então para o nosso Q&A. Agradeço aí o tempo e a atenção'\n",
        "#bbdc\n",
        "df_divided.iloc[185,6]  = 'Muito obrigado pela participação de todos que estão conectados em nossa teleconferência, e a partir de agora estamos à disposição'\n",
        "df_divided.iloc[70,6]  = 'Obrigada. Iniciaremos agora a sessão de perguntas e respostas. Para fazer uma pergunta, por favor, digitem “*1”, e se quiser retirar a pergunta da lista'\n",
        "df_divided.iloc[208,6]  = 'Muito obrigado pela atenção das senhoras e dos senhores e passamos agora para a seção de perguntas e respostas. Muito obrigado'\n",
        "df_divided.iloc[218,6]  = 'Obrigado pela atenção de todos até o momento e passamos agora para a seção de perguntas e respostas'\n",
        "df_divided.iloc[135,6]  ='Finalizo por aqui essa primeira parte da reunião e vou me juntar ao Firetti no outro estúdio para seguirmos para a sessão de perguntas e respostas'\n",
        "#bmgb\n",
        "df_divided.iloc[110,6]  = 'Com isso eu finalizo a nossa apresentação, nossos slides, e abrimos agora para perguntas de vocês.'\n",
        "df_divided.iloc[287,6]  = 'Com isso, encerro aqui a apresentação. Agora vamos abrir para as perguntas de vocês.'\n",
        "#brsr\n",
        "df_divided.iloc[68,6]  = 'Eu gostaria de encerrar por enquanto, e colocar então à disposição para eventuais perguntas que os analistas tenham a nos fazer. Muito obrigado até o momento.'\n",
        "df_divided.iloc[210,6]  = 'São dois comentários importantes que eu gostaria de fazer antes de encaminhar as perguntas e respostas. Nós estamos então à disposição às perguntas dos analistas'\n",
        "df_divided.iloc[154,6]  = 'Essas foram algumas considerações básicas que gostaríamos de fazer, e estamos à disposição, então, para eventuais perguntas. Obrigado.'\n",
        "df_divided.iloc[242,6]  = 'Com isso concluímos a apresentação dos resultados desse 1S. Quero agradecer mais uma vez o interesse e a atenção de todos, e dizer que a partir'\n",
        "df_divided.iloc[228,6]  = 'nossa área de RI em contato permanente para poder fornecer dados também por escrito, via email, etc. Então, aguardamos as questões dos senhores.'\n",
        "df_divided.iloc[158,6]  = 'aos senhores e senhoras, para que possamos atendê-los nas demandas que houver. Vamos para a sessão de perguntas e respostas, então. Obrigado.'\n",
        "df_divided.iloc[100,6]  = 'Então, basicamente é isso. Agradeço a todos a presença e aguardo questionamentos. Muito obrigado.'\n",
        "df_divided.iloc[231,6]  = 'Iniciaremos nossa sessão de perguntas e respostas, que contará com a participação do Sr.Nathan Meneguzzi'\n",
        "#itub\n",
        "df_divided.iloc[151,6]  = 'Acho que é isso. Podemos abrir aqui para perguntas. Então, vamos a elas.'\n",
        "df_divided.iloc[336,6]  = 'E, com isso, eu termino a apresentação de resultados e abro para perguntas e respostas'\n",
        "df_divided.iloc[326,6]  = 'Está bom? Muito obrigado. Vou subir agora para o estúdio e vou encontrar com o Renato para que a gente possa continuar o nosso bate pa'\n",
        "#nu\n",
        "df_divided.iloc[302,6]  = \"Não existe\"\n",
        "df_divided.iloc[124,6]  = 'Operadora: Iniciaremos agora a sessão de perguntas e respostas para investidores e analistas'\n",
        "#prbc\n",
        "df_divided.iloc[63,6]  = \"Não Existe\"\n",
        "df_divided.iloc[298,6]  = \"Não Existe\"\n",
        "df_divided.iloc[35,6]  = \"Não Existe\"\n",
        "df_divided.iloc[174,6]  = \"Não Existe\"\n",
        "df_divided.iloc[328,6]  = \"Não Existe\"\n",
        "df_divided.iloc[198,6]  = \"Não Existe\"\n",
        "#sanb\n",
        "df_divided.iloc[55,6]  = 'Sem mais, eu passo a palavra ao moderador.'\n",
        "df_divided.iloc[257,6]  = 'Muito obrigado pela sua participação, e agora abro para a sessão de perguntas e respostas.'\n",
        "df_divided.iloc[108,6]  = 'Muito obrigado e estamos disponíveis para responder perguntas agora.'\n",
        "df_divided.iloc[211,6]  = 'Com isso, eu concluo a minha apresentação. Acho que o André e eu podemos passar para a sessão de perguntas e respostas'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPx-PLcMaC03"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# Example folder paths\n",
        "transcript_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Transcripts/'\n",
        "transcript_text_folder = \"/content/drive/MyDrive/Portfolio Projects/Mestrado/transcricoes_processadas_BERTopic/\"\n",
        "\n",
        "# Suppose these are your files\n",
        "files = os.listdir(transcript_folder)\n",
        "files = files#[:10]  # Just first 10 as an example\n",
        "\n",
        "def process_single_file(file_name, df_cutoff_merge):\n",
        "    \"\"\"\n",
        "    Worker function that processes a single file.\n",
        "    Returns a dictionary with doc name, 'presentation', 'qna' segments, and their lengths.\n",
        "    \"\"\"\n",
        "    path_transcription = os.path.join(transcript_folder, file_name)\n",
        "\n",
        "    # Split file name to extract relevant parts\n",
        "    file_name_parts = file_name.split(\"-\")\n",
        "    ticker    = file_name_parts[0].strip()\n",
        "    ano       = file_name_parts[1].strip()\n",
        "    trimestre = file_name_parts[2].strip()\n",
        "\n",
        "    # Decide on how to call raw_text_extract based on ticker\n",
        "    if ticker in ['bbas', 'bbdc']:\n",
        "        page_data = raw_text_extract(path_transcription)\n",
        "    else:\n",
        "        page_data = raw_text_extract(path_transcription, include_tables=True, include_images=False)\n",
        "\n",
        "    # JOIN the list of lines/pages into a single string\n",
        "    text = \" \".join(page_data)\n",
        "\n",
        "    # Build the doc_name for lookup in df_cutoff_merge\n",
        "    doc_name = f'{ticker}-{ano}-{trimestre[0]}'\n",
        "\n",
        "    # Retrieve the cutoff pattern for this doc\n",
        "    # IMPORTANT: Make sure doc_name actually matches something in df_cutoff_merge\n",
        "    # e.g. 'bbas-:2023-1'\n",
        "    cutoff_rows = df_divided[df_divided['doc'] == doc_name]\n",
        "    if len(cutoff_rows) == 0:\n",
        "        # If no matching row, skip or handle logic\n",
        "        # For now, treat entire text as presentation\n",
        "        return {\n",
        "            'doc': doc_name,\n",
        "            'presentation': text,\n",
        "            'qna': None,\n",
        "            'presentation_len': len(text),\n",
        "            'qna_len': 0\n",
        "        }\n",
        "\n",
        "    cutoff_str = cutoff_rows['cutoff_revised'].values[0]\n",
        "    print(doc_name, cutoff_str)\n",
        "\n",
        "    # Escape special regex chars to avoid unbalanced parentheses or similar\n",
        "    pattern = re.escape(cutoff_str)\n",
        "\n",
        "    # Search for the pattern in the text\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        split_index = match.start()\n",
        "        presentation = text[:split_index].strip()\n",
        "        qna = text[split_index:].strip()\n",
        "    else:\n",
        "        # If no match, entire text is 'presentation'\n",
        "        presentation = text\n",
        "        qna = None\n",
        "\n",
        "    # Create a row-like dictionary\n",
        "    new_row = {\n",
        "        'doc': doc_name,\n",
        "        'cutoff': cutoff_str,\n",
        "        'presentation': presentation,\n",
        "        'qna': qna,\n",
        "        'presentation_len': len(presentation),\n",
        "        'qna_len': len(qna) if qna else 0\n",
        "    }\n",
        "    return new_row\n",
        "\n",
        "# Prepare an empty DataFrame to gather results\n",
        "df_divided_revised = pd.DataFrame(columns=['doc','presentation','qna','presentation_len','qna_len'])\n",
        "\n",
        "num_files = len(files)\n",
        "texts_as_strings = []\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    # Use functools.partial to pass df_cutoff_merge\n",
        "    process_func = partial(process_single_file, df_cutoff_merge=df_cutoff_merge)\n",
        "\n",
        "    # Executor map returns an iterator of dictionaries (one per file)\n",
        "    results = executor.map(process_func, files)\n",
        "\n",
        "    for i, row_dict in enumerate(results, start=1):\n",
        "        print(f\"Processed file {i} of {num_files}\")\n",
        "\n",
        "        # row_dict is the dictionary returned by process_single_file\n",
        "        # We can build texts_as_strings if needed\n",
        "        presentation_text = row_dict['presentation'] if row_dict['presentation'] else \"\"\n",
        "        qna_text = row_dict['qna'] if row_dict['qna'] else \"\"\n",
        "        combined_text = presentation_text + \" \" + qna_text\n",
        "        texts_as_strings.append(combined_text)\n",
        "\n",
        "        # Append row to df_divided\n",
        "        df_divided_revised = pd.concat([df_divided_revised, pd.DataFrame([row_dict])], ignore_index=True)\n",
        "\n",
        "print(\"All files processed.\")\n",
        "print(df_divided_revised.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqaliUj0aJ-T"
      },
      "outputs": [],
      "source": [
        "general_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Divided_text/'\n",
        "if not os.path.exists(general_folder):\n",
        "    os.mkdir(general_folder)\n",
        "\n",
        "transcript_presentation_folder = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Divided_text/presentation/'\n",
        "transcript_qna_folder          = '/content/drive/MyDrive/Portfolio Projects/Mestrado/Divided_text/qna'\n",
        "\n",
        "if not os.path.exists(transcript_presentation_folder):\n",
        "    os.mkdir(transcript_presentation_folder)\n",
        "if not os.path.exists(transcript_qna_folder):\n",
        "    os.mkdir(transcript_qna_folder)\n",
        "\n",
        "count = 0\n",
        "num_files = len(df_divided_revised)\n",
        "for index, row in df_divided_revised.iterrows():\n",
        "  count += 1\n",
        "  file_name = f\"{row['doc']}.txt\"\n",
        "  print(f'Processing file {count} of {num_files}')\n",
        "\n",
        "  file_path_presentation = os.path.join(transcript_presentation_folder, file_name)\n",
        "  with open(file_path_presentation, 'w') as file:\n",
        "    file.write(row['presentation'])\n",
        "\n",
        "  if row['qna'] is not None:\n",
        "    file_path_qna = os.path.join(transcript_qna_folder, file_name)\n",
        "    with open(file_path_qna, 'w') as file:\n",
        "      file.write(row['qna'])\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "print('All files processed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbyEvQ3baPYq"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXNeVjuCaQtd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
